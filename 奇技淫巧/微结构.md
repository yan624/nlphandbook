## Dropout
dropout 以 $p$ 的概率丢掉输出，并且原始版本需要在推理阶段对激活值同比缩小 p。不过这样操作起来很不方便，在推理阶段需要根据训练阶段的 dropout 策略，改动对应代码。因此提出了 inverted dropout，只在训练阶段改动。对每个激活值乘上 1/p，对应的梯度也乘上 1/p。这样，在推理阶段就可以无数 dropout 策略了，直接关闭即可。进行相应的缩放主要是为了保证在训练阶段和推理阶段之间，激活值的数学期望是一样的。

此外，还有一种 dropconnect 以 $p$ 的概率丢掉权重。

**为什么 BN 不和 dropout 一起用？** 一起用会产生方差偏移，即训练时计算的方差和测试时的不一样。BN 正是依赖方差进行标准化。

Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift

## Normalization
**为什么 Normalization 有效？**目前有几种主流观点：

1. 我们首先知道在机器学习中标准化输入特征可以加速学习，由此推理对 hidden layer 应当也奏效。（均值是 0，方差是 1 的分布对激活函数很友好。对于 sigmoid 或 tanh，如果不标准化会让大部分激活值都处于饱和阶段，即不是 -1 就是 1）
2. 深度学习一直有一个挑战：一层权重的梯度高度依赖前一层神经元的输出（即激活值），见 @Ba2016 第二节。例如线性变换 $Wa + b$，对权重求导，可以发现前面有一个激活值系数。*这表明想要减小协变量偏移问题可以通过固定每层的均值和方差实现，即归一化（标准化）。*
3. 对学习率的敏感度更低。
4. **Batch Norm**（其它不是）可以轻微地被视为正则化。在 Batch Norm 中，均值和方差来自小批量，所以均值和方差包含了噪音（noise）。正如同 dropout 对隐藏层的激活函数加入了噪音。

**由于这一直没有定论，以下再引用几篇最近论文的观点。**

[How Does Batch Normalization Help Optimization?](https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf)

[Understanding and Improving Layer Normalization](https://proceedings.neurips.cc/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf)

[PowerNorm: Rethinking Batch Normalization in Transformers](https://arxiv.org/pdf/2003.07845.pdf)

### Layer Normalization
Layer Normalization 主要解决两个问题：1）协变量偏移（covariate shift）；2）Batch Norm 很难应用在 NLP 领域。

Batch Norm 具有一些缺点：1）Batch Norm 需要对一批数据求均值和方差，当批次大小不大时，归一化效果不明显。2）此外，在 NLP 领域使用 Batch Norm 也很奇怪，一个批次中的不同文本序列不见得有关系。例如“猫坐在椅子上”和“苹果落地”，“猫”和“苹”并没有任何关系，即[分布不同](https://www.zhihu.com/question/308310065/answer/1746317308)。3）而且，这两句话的长度还不同。

Layer Norm 对特征进行归一化，每个样本都有自己的均值和方差。公式为：

$$
\begin{aligned}
	h^t & = f[\frac{g}{\sigma^t} \odot (a^t - \mu^t) + b] \\
	\mu^t & = \frac{1}{H} \sum^H_{i=1} a^t_i \\
	\sigma^t & = \sqrt{\frac{1}{H} \sum^H_{i=1} (a^t_i - \mu^t)^2}
\end{aligned}
$$

其中 $H$ 代表特征维度大小，$mu^t$ 是第 $t$ 个时间步的均值，$\sigma^t$ 方差。$g, b$ 分别是收益（gain）和偏差参数，维度与 $h^t$ 相同。

**注意：$a^t$ 并不是激活值，而是总和输入（summed input） $wx$。**例如，当使用线性变换时，$a^t$ 是谁不言而喻，Layer Norm 中的 $b$ 就和线性变换的 $b$ 合并了。使用 self-attention 时，$a^t$ 指的是残差网络的输出，无论是残差网络还是多头注意力机制都是没有偏差项 $b$ 的，那么 Layer Norm 的 $b$ 就属于它本身。

**Layer Norm 为什么最后要缩放+偏移？**这是为了让模型还能回到原来的分布，就是抵消方差和均值的影响。如果模型认为前面的标准化是无用的，就利用这两个参数来抵消之前的操作。

**Layer Norm 为什么有效（优点）？**其实就是 Normalization 为什么有效的问题。

## 残差网络
残差网络（Residual Network，ResNet）现在被认为用于解决网络退化问题，即当使用更深的网络时，其训练和测试误差反而比略浅网络还要高。这不符合常理。因为多余的那几层网络只要是恒等映射，那么这两个模型应该能取得一致的结果。一个合理的猜测是，神经网络的恒等映射并不容易学到。

既然学不到，就自己构建一个。残差网络的公式是：

$$a^l = a^{l-1} + f(a^{a-l})
$$

其中 $f(a^{a-l})$ 是残差函数，只需要让 $f(a^{a-l}) \rightarrow 0$，就构建了恒等映射。

此外，也有人认为残差网络有效是因为它类似于集成学习。

**残差网络为什么有效？**知乎文章[《残差网络解决了什么，为什么有效？》](https://zhuanlan.zhihu.com/p/80226180)引用三篇论文的观点（前后向信息传播、集成学习、梯度破碎）做了一番解释。*有新论文再补充。*



## 参考文献
- Dropout
	1. [神经网络 Dropout 层中为什么 dropout 后还需要进行 rescale？](https://www.zhihu.com/question/61751133)
	2. [大白话 《Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift》](https://zhuanlan.zhihu.com/p/33101420)
- Normalization
	1. [什么是批标准化 (Batch Normalization)](https://zhuanlan.zhihu.com/p/24810318)以及文中评论
	2. [论文| How Does Batch Normalizetion Help Optimization](https://zhuanlan.zhihu.com/p/66683061)
- ResNet
	1. [残差网络解决了什么，为什么有效？](https://zhuanlan.zhihu.com/p/80226180)
<textarea id="bibtex_input" style="display:none;">
@Article{Ba2016,
  author        = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal       = {arXiv preprint arXiv:1607.06450},
  title         = {Layer Normalization},
  year          = {2016},
  month         = jul,
  abstract      = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  eprint        = {1607.06450},
  file          = {:pdf/tricks/1607.06450_Layer Normalization.pdf:PDF},
  groups        = {tricks},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}
</textarea>

