?> æœ¬æ–‡é¦–å…ˆä»¥ BERT ä¸ºä¾‹ï¼Œæè¿°å…¶ğŸ“”è¾“å…¥ä¸åµŒå…¥è¡¨ç¤ºã€ğŸ§¬æ¨¡å‹ç»“æ„ã€âš½ä¼˜åŒ–ç›®æ ‡ã€ğŸºå¦‚ä½•ä½¿ç”¨ BERT çš„éšè—çŠ¶æ€ä»¥åŠğŸæ”¹è¿›ä¸ç¼ºé™·ã€‚æœ€åé˜è¿°å¯¹ BERT çš„ç–‘é—®ï¼ˆğŸš€è‡ªé—®è‡ªç­”ï¼‰ï¼Œç„¶åå¯¹æœ¬æ–‡è¿›è¡Œæ€»ç»“ã€‚  
[BERT è®ºæ–‡åœ°å€](https://arxiv.org/pdf/1810.04805.pdf)ï¼Œ[é¢„è®­ç»ƒæ¨¡å‹åœ°å€](https://github.com/google-research/bert#pre-trained-models)

## ğŸ”–å¼• è¨€
BERT å…±åˆ†ä¸ºä¸¤æ­¥ï¼šé¢„è®­ç»ƒï¼ˆpre-trainingï¼‰å’Œå¾®è°ƒï¼ˆfine-tuningï¼‰ã€‚é¢„è®­ç»ƒé˜¶æ®µï¼ŒBERT ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®å¯¹ä¸åŒçš„ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼ŒBERT æ¨¡å‹é¦–å…ˆåŠ è½½é¢„è®­ç»ƒå‚æ•°ï¼Œç„¶åä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„æ ‡ç­¾æ•°æ®å¾®è°ƒæ‰€æœ‰å‚æ•°ã€‚æ¯ç§ä»»åŠ¡éƒ½æœ‰ç‹¬ç«‹çš„å¾®è°ƒæ¨¡å‹ï¼Œå³ä½¿å®ƒä»¬ä½¿ç”¨ç›¸åŒçš„é¢„è®­ç»ƒå‚æ•°[[@devlin2018bert]](#devlin2018bert)ã€‚è¿™é‡Œçš„å¾®è°ƒæ¨¡å‹åº”è¯¥æŒ‡çš„æ˜¯ä¸‹æ¸¸æ¨¡å‹æ¶æ„ï¼Œæ¯”å¦‚åˆ†ç±»æ¨¡å‹ä½¿ç”¨ [CLS] è¿›è¡Œçº¿æ€§åˆ†ç±»ï¼Œåºåˆ—æ ‡æ³¨ä½¿ç”¨åºåˆ—çš„çŠ¶æ€è¾“å‡ºã€‚

BERT çš„ç‰¹æ€§æ˜¯ï¼šå¯¹äºä¸åŒä»»åŠ¡ï¼Œé¢„è®­ç»ƒç»“æ„ä¸æœ€ç»ˆä¸‹æ¸¸ä»»åŠ¡ç»“æ„åªå…·æœ‰å¾®å°çš„å·®å¼‚ï¼Œä½†æ˜¯**æ•´ä½“ä¸Šå‡ ä¹æ˜¯ç»Ÿä¸€çš„**ã€‚

BERT å¹¶ä¸æ˜¯ä¸‡èƒ½çš„ï¼Œå®ƒæ— æ³•å®Œæˆè‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ‰€æœ‰ä»»åŠ¡ï¼Œå…¶é€‚ç”¨çš„ä»»åŠ¡ä¸ºï¼šè‡ªç„¶è¯­è¨€ç†è§£ï¼ˆåˆ†ç±»ã€æ¨ç†ï¼‰ã€é—®ç­”ï¼ˆspan-based or notï¼‰ã€SWAG é€»è¾‘è¿ç»­æ€§æ¨ç†ã€‚

## ğŸ’‰è¾“å…¥ä»¥åŠåµŒå…¥çš„è¡¨ç¤º
ä¸ºäº†å¤„ç†å¤šæ ·çš„ä¸‹æ¸¸ä»»åŠ¡ï¼ŒBERT çš„è¾“å…¥â€œè¯­å¥â€å¯ä»¥æ˜¯ä¸€æ®µä»»æ„è·¨åº¦çš„è¿ç»­æ–‡æœ¬ï¼Œå¹¶éå¿…é¡»ä¸ºä¸€æ¡çœŸå®çš„è¯­å¥ã€‚â€œè¯­å¥â€å¯ä»¥è¡¨ç¤ºä¸ºå•æ¡è¯­å¥æˆ–ä¸€å¯¹è¯­å¥çš„è¯­ç´ åºåˆ—ã€‚

BERT çš„åµŒå…¥ä¸ RNN ç³»åˆ—æ¨¡å‹æœ‰æ‰€ä¸åŒï¼Œå…±ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼Œåˆ†åˆ«ä¸ºè¯åµŒå…¥ï¼ˆword embeddingï¼‰ã€ä½ç½®åµŒå…¥ï¼ˆposition embeddingï¼‰ä»¥åŠç‰‡æ®µåµŒå…¥ï¼ˆsegment embeddingï¼‰ã€‚è¿™æ˜¯å› ä¸º BERT æ— æ³•æ•è·åºåˆ—çš„è¯­åºä¿¡æ¯ï¼Œéœ€è¦ä½¿ç”¨ä½ç½®åµŒå…¥è¡¨ç¤ºå•è¯é¡ºåºï¼Œä½¿ç”¨ç‰‡æ®µåµŒå…¥è¡¨ç¤ºå¥å­é¡ºåºã€‚åœ¨è®¡ç®—æ—¶ï¼ŒBERT é‡‡ç”¨å°†ä¸‰è€…ç›´æ¥ç›¸åŠ çš„æ–¹å¼ä»è€Œäº§ç”Ÿä¸€ä¸ªå…¨æ–°çš„åµŒå…¥ã€‚ä¸‹é¢å°†åˆ†åˆ«ä»‹ç»ä¸‰ç§åµŒå…¥ã€‚

?> ç›¸è¾ƒäº BERTï¼ŒRNN ç³»åˆ—æ¨¡å‹å…·æœ‰æ•è·åºåˆ—è¯­åºä¿¡æ¯çš„å¤©ç„¶ä¼˜åŠ¿ï¼Œå®ƒä»¬ä¼šé¡ºåºåœ°å¤„ç†å•è¯å¹¶ç”Ÿæˆéšè—çŠ¶æ€ï¼Œå…¶ä¸­æ¯ä¸ªéšè—çŠ¶æ€éƒ½åŒ…å«äº†æ—¶åºä¿¡æ¯ã€‚å¦‚æœå¥ä¸­å•è¯çš„é¡ºåºå‘ç”Ÿå˜åŒ–ï¼Œé‚£ä¹ˆå•è¯çš„éšè—çŠ¶æ€ä¹Ÿä¼šæœ‰æ‰€å˜åŒ–ã€‚

**è¯åµŒå…¥**æœ‰ä¸¤ç§åˆå§‹åŒ–æ–¹å¼ï¼š1ï¼‰éšæœºåˆå§‹åŒ–å¹¶å¯¹å…¶ä¼˜åŒ–ï¼›2ï¼‰ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œä¾‹å¦‚ GloVeã€word2vecã€‚æ— è®ºé‡‡ç”¨ä»¥ä¸Šå“ªç§æ–¹å¼ï¼Œè¾“å…¥åºåˆ—å‡ä¸ºå„å•è¯çš„ one-hot ç¼–ç ï¼Œä»¥æŸ¥è¡¨çš„æ–¹å¼ä» BERT çš„åµŒå…¥å±‚ä¸­å¾—åˆ°è¯å‘é‡ã€‚BERT é‡‡ç”¨è¯è¡¨å¤§å°ä¸º 30000 çš„ WordPiece embeddings åˆå§‹åŒ–æ‰€æœ‰è¯åµŒå…¥ã€‚

**ä½ç½®åµŒå…¥**ï¼šç”±äº BERT å¤©ç„¶ä¸å…·æœ‰æ•è·åºåˆ—é¡ºåºä¿¡æ¯çš„èƒ½åŠ›ï¼Œå› æ­¤å¦‚ä½•è¡¨ç¤ºä½ç½®åµŒå…¥å°±æ˜¾å¾—è‡³å…³é‡è¦ã€‚å…¶å…·æœ‰å¤šç§è¡¨ç¤ºæ–¹æ³•ï¼šTransformer ä¸»è¦å°è¯•ä¸¤ç§ï¼Œå³è®­ç»ƒå¥½çš„åµŒå…¥ä»¥åŠæ­£å¼¦ä½ç½®åµŒå…¥ã€‚BERT é‡‡ç”¨é¢„å…ˆè®­ç»ƒçš„æ–¹å¼ã€‚

> æ›´å¤šå…³äºä½ç½®åµŒå…¥çš„è®¨è®ºè¯¦è§ Transformer ä¸€ç« ã€‚

**ç‰‡æ®µåµŒå…¥**ï¼šBERT ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒå¥½çš„åµŒå…¥[[@devlin2018bert]](#devlin2018bert)ã€‚

## ğŸ§¬æ¨¡å‹ç»“æ„
BERT çš„æ¨¡å‹ç»“æ„ä½¿ç”¨å¤šå±‚åŒå‘çš„ Transformer encoderï¼Œè®ºæ–‡ä¸­ä½¿ç”¨ [tensor2tensor](https://github.com/tensorflow/tensor2tensor) åº“è¿›è¡Œå®ç°ï¼Œä»–ä»¬å‘å¸ƒäº†å¤šä¸ªç‰ˆæœ¬çš„ BERTï¼Œå…¶ä¸­ $BERT_{BASE}$ çš„å‚æ•°ä¸ GPT ä¸€è‡´ï¼Œä¸»è¦ä¸ºäº†ä¸å…¶å¯¹æ¯”ã€‚

## âš½ä¼˜åŒ–ç›®æ ‡
ä¸ ELMo å’Œ GPT ä¸åŒï¼ŒBERT æ²¡æœ‰ä½¿ç”¨ä¼ ç»Ÿçš„ä»å·¦è‡³å³æˆ–è€…ä»å³è‡³å·¦çš„è¯­è¨€æ¨¡å‹ã€‚è€Œæ˜¯ä½¿ç”¨ä¸¤ç§å…¶ä»–çš„æ— ç›‘ç£ä»»åŠ¡ï¼Œåˆ†åˆ«ä¸º Masked Language Modelï¼ˆMLMï¼‰ ä»¥åŠ  Next Sentence Predictionï¼ˆNSPï¼‰ã€‚

### Masked LM
ç›´è§‰ä¸Šæ¥è®²ï¼Œæœ‰ç†ç”±ç›¸ä¿¡**æ·±å±‚åŒå‘æ¨¡å‹**æ¯”**ä»å·¦è‡³å³**æˆ–è€…**æµ…å±‚çš„ä»å·¦è‡³å³ä»¥åŠä»å³è‡³å·¦çš„æ‹¼æ¥æ¨¡å‹**è¦å¥½ã€‚**ä¸å¹¸çš„æ˜¯ï¼Œæ ‡å‡†çš„è¯­è¨€æ¨¡å‹åªèƒ½åšåˆ°æ•è·ä¸€ä¸ªæ–¹å‘çš„è¯­è¨€ç‰¹å¾**ï¼Œ*åŸå› å¦‚ä¸‹ï¼Œä½†æ˜¯æˆ‘è®¤ä¸ºè¿™ä¸¤å¥è¯æ²¡æœ‰å› æœå…³ç³»å§ï¼Ÿ*

> Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly â€œsee itselfâ€, and the model could trivially predict the target word in a multi-layered context.

ä¸ºäº†è®­ç»ƒä¸€ä¸ªæ·±å±‚çš„åŒå‘è¯­è¨€æ¨¡å‹ï¼ŒBERT ä»¥ä¸€å®šæ¦‚ç‡ï¼ˆè®ºæ–‡ä¸­ä¸º 15%ï¼‰éšæœºåœ°æ©ç›–è¾“å…¥è¯­å¥ä¸­çš„éƒ¨åˆ†è¯­ç´ ï¼ˆè¯­ç´ æŒ‡ wordpiece tokenï¼‰ï¼Œç„¶åé¢„æµ‹è¿™äº›è¯­ç´ ã€‚è¿™è¢«ç§°ä¸º MLMï¼Œåœ¨æ–‡å­¦é¢†åŸŸè¿™è¢«ç§°ä¸ºå®Œå½¢å¡«ç©ºï¼ˆClozeï¼‰ã€‚åŸºäºè¯¥åšæ³•ï¼Œè¢«æ©ç›–è¯­ç´ çš„æœ€ç»ˆéšè—å‘é‡è¢«è¾“å…¥ä¸€ä¸ªåŸºäºè¯è¡¨å¤§å°çš„ softmaxï¼Œè¿™ä¸æ ‡å‡†çš„è¯­è¨€æ¨¡å‹ç›¸åŒã€‚ä¸ denoising auto-encoders (Vincent et al., 2008) ç›¸æ¯”ï¼ŒBERT åªæ˜¯é¢„æµ‹è¢«æ©ç›–çš„å•è¯ï¼Œè€Œä¸æ˜¯é‡æ„æ•´ä¸ªè¾“å…¥ã€‚

å°½ç®¡åŸºäºMLMçš„BERTæ‹¥æœ‰äº†å¼ºå¤§çš„æ€§èƒ½ï¼Œç„¶è€Œå´å¼•å…¥äº†ä¸€ä¸ªç¼ºç‚¹ï¼Œå³**é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µçš„ä¸ä¸€è‡´æ€§ï¼Œè¿™æ˜¯å› ä¸º `[MASK]` æ²¡æœ‰å‡ºç°åœ¨å¾®è°ƒé˜¶æ®µ**ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼ŒBERT ä¸æ€»æ˜¯ä½¿ç”¨ `[MASK]` æ›¿æ¢å•è¯ã€‚è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ä»¥ 15% çš„æ¦‚ç‡é€‰æ‹©ä¸€ä¸ªè¯­ç´ ä½ç½®ï¼Œåœ¨è¿›è¡Œæ©ç›–æ—¶åˆ†ä¸ºä»¥ä¸‹ä¸‰ç§æƒ…å†µï¼šï¼ˆ1ï¼‰80% æ›¿æ¢ï¼›ï¼ˆ2ï¼‰10% æ›¿æ¢æˆä¸€ä¸ªéšæœºçš„è¯­ç´ ï¼›ï¼ˆ3ï¼‰10% ä¸åšå˜åŒ–ã€‚ç„¶å $T_i$ å°†è¢«ç”¨äºé¢„æµ‹çœŸå®è¯­ç´ ï¼Œå…¶åŸºäºäº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚ä»¥ä¸Šä¸‰ç§æ–¹å¼å„ç§å˜ä½“çš„å¯¹æ¯”è¯¦è§é™„å½• C.2ã€‚

### NSP
è®¸å¤šé‡è¦çš„ä¸‹æ¸¸ä»»åŠ¡éƒ½éœ€è¦ç†è§£ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»ï¼Œä¾‹å¦‚ QA å’Œ NLIï¼Œè¿™å¹¶ä¸æ˜¯è¯­è¨€æ¨¡å‹èƒ½æ•æ‰åˆ°çš„ã€‚ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿç†è§£å¥å­ä¹‹é—´çš„è”ç³»ï¼ŒBERT è¿˜é¢å¤–è®­ç»ƒäº† NSP ä»»åŠ¡ã€‚å…·ä½“æ¥è®²ï¼Œå¯¹äºå¥å­ A å’Œ Bï¼Œ50% çš„æƒ…å†µä¸‹ B å°±æ˜¯ A çš„ä¸‹ä¸€å¥ï¼ˆæ ‡ç­¾ä¸º `IsNext`ï¼‰ï¼›50% çš„æƒ…å†µä¸‹ A çš„ä¸‹ä¸€å¥æ˜¯ä¸€æ¡éšæœºè¯­å¥ï¼ˆæ ‡ç­¾ä¸º `NotNext`ï¼‰ã€‚

## ğŸºå¦‚ä½•ä½¿ç”¨éšè—çŠ¶æ€
BERT ä½¿ç”¨çš„æ¡†æ¶åˆ†ä¸ºä¸¤æ­¥ï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œè¿™ä¸ ELMo ä¸åŒï¼Œå®ƒä½¿ç”¨çš„æ˜¯é¢„è®­ç»ƒå’ŒåŸºäºç‰¹å¾ï¼ˆfeature-basedï¼‰ã€‚æ‰€ä»¥ ELMo éœ€è¦å†³å®šä½¿ç”¨å“ªå±‚çš„éšè—çŠ¶æ€ä½œä¸ºä¸‹æ¸¸ä»»åŠ¡çš„è¾“å…¥ï¼Œè€Œ BERT åªéœ€è¦åœ¨æ¨¡å‹çš„é¡¶éƒ¨åŠ ä¸Šä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹ï¼Œç„¶åç»§ç»­å¾®è°ƒå³å¯ã€‚

## ğŸæ”¹è¿›ä¸ç¼ºé™·
æ”¹è¿›ï¼š  
1. ä½¿ç”¨åŒå‘ Transformer
2. å¼•å…¥ MLM
3. GELU

ç¼ºé™·ï¼š  
1. è¾“å…¥æ˜¯ wordpieceï¼Œè¿™è®©åŸºäº span çš„ä»»åŠ¡å¾ˆéš¾åŠã€‚å¯¹äºå‘½åä½“è¯†åˆ«æ¥è¯´ï¼Œæ¯ä¸€ä¸ªå•è¯éƒ½æœ‰å¯¹åº”çš„æ ‡ç­¾ï¼Œå¦‚æœç”¨ wordpiece ç®—æ³•ä¼šæ‰“ä¹±æ ‡ç­¾ã€‚BERT çš„åšæ³•æ˜¯åªä½¿ç”¨ sub-word çš„ç¬¬ä¸€ä¸ªéƒ¨åˆ†çš„éšè—çŠ¶æ€ï¼Œè¯¦æƒ…å¯å‚è€ƒ[issue1](https://github.com/huggingface/transformers/issues/323)ã€‚éœ€è¦æ³¨æ„ï¼ŒBERT çš„è®ºæ–‡åº”è¯¥åšè¿‡ä¿®æ”¹ï¼Œissue1 ä¸­æåˆ°çš„æ®µè½ï¼Œç°åœ¨åº”è¯¥åœ¨ 5.3ã€‚åŸæ–‡æˆ‘æ”¾åœ¨ä¸‹é¢ã€‚ç„¶è€Œè¿™åªèƒ½è§£å†³å‘½åä½“è¯†åˆ«ä»»åŠ¡ï¼Œæ— æ³•è§£å†³åŸºäºä½ç½®ä¿¡æ¯çš„ä»»åŠ¡ï¼Œä¾‹å¦‚åŸºäº index-based Ptr æ¨¡å‹åšçš„ä»»åŠ¡ã€‚
	- > We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.
1. æœ€å¤§é•¿åº¦é™åˆ¶ä¸º 512ï¼Œæ— æ³•å¤„ç†é•¿æ–‡æœ¬ã€‚å³ä½¿èƒ½å¤„ç†ä¹Ÿæ˜¯ç‹—å°¾ç»­è²‚ï¼Œæœ‰äº›è®ºæ–‡ä¸­ä½¿ç”¨é¢å¤–çš„æŠ€å·§è¿›è¡Œå¤„ç†ï¼Œä½†æ˜¯è¿™æ ·å°±æ¯”è¾ƒå¼ºè¡Œã€‚
2. æé«˜é¢„æµ‹å‡†ç¡®ç‡åˆ°åº•æ¥æºäºé¢„è®­ç»ƒï¼Ÿè¿˜æ˜¯æ•°æ®é‡å¤§ï¼Ÿè¿˜æ˜¯ä¾é çš„åº•å±‚æ¨¡å‹ï¼Ÿ
3. åªèƒ½è¾“å…¥ä¸¤å¥è¯ï¼Ÿ
4. ç”±äº Transformer å®Œå…¨æ²¡æœ‰è¯­åºçš„æ¦‚å¿µï¼Œæ‰€ä»¥éœ€è¦ç”¨ä½ç½®ç¼–ç ã€‚
5. ä»»åŠ¡æ–¹é¢ï¼šæ— æ³•å®Œæˆè¯­è¨€ç”Ÿæˆä»»åŠ¡
7. ç¡¬ä»¶æ–¹é¢ï¼šæ¨¡å‹è¿‡äºåºå¤§ï¼Œå¦‚æœä½¿ç”¨ BERT æå–ç‰¹å¾ï¼Œæ¥ä¸‹æ¥çš„åˆ†ç±»å±‚åªèƒ½è®¾è®¡å°ä¸€ç‚¹

## ğŸ“”æ€»ç»“
1. BERT ç”±é¢„è®­ç»ƒä»¥åŠå¾®è°ƒä¸¤æ­¥ç»„æˆï¼Œé¢„è®­ç»ƒæ­¥éª¤ä½¿ç”¨å¤§è§„æ¨¡çš„æ— æ ‡ç­¾æ–‡æœ¬æ•°æ®ä¼˜åŒ–å‚æ•°ï¼Œå¾®è°ƒæ­¥éª¤ä½¿ç”¨æ ‡æ³¨æ•°æ®ä¼˜åŒ–é¢„è®­ç»ƒè¿‡çš„å‚æ•°ã€‚
2. BERT çš„åµŒå…¥ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼Œåˆ†åˆ«ä¸ºè¯­ç´ ã€ä½ç½®ä»¥åŠç‰‡æ®µã€‚åœ¨æ¨¡å‹è¿›è¡Œè®¡ç®—æ—¶ï¼Œå°†ä¸‰è€…ç›¸åŠ ã€‚
3. æ¨¡å‹çš„ç»“æ„é‡‡ç”¨å¤šå±‚åŒå‘çš„ Transformer Encoderï¼Œè€ŒéELMo ä½¿ç”¨çš„å¤šå±‚â€œåŒå‘â€ LSTM æˆ–è€… GPT çš„å¤šå±‚å•å‘ Transformer Decoderã€‚è¿™æ˜¯å› ä¸ºä»ç›´è§‰ä¸Šæ¥è®²ï¼Œæ·±å±‚åŒå‘æ¨¡å‹å¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚ELMo çš„åŒå‘æ¨¡å‹åªæ˜¯å°†ä¸¤ä¸ªå•å‘æ¨¡å‹è¿›è¡Œæ‹¼æ¥ã€‚ä¸å¹¸çš„æ˜¯ï¼Œä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹åªèƒ½æ•è·ä¸€ä¸ªåå‘çš„ä¿¡æ¯ï¼Œä¸ºæ­¤å¼•å…¥ Masked Language Modelï¼ˆMLMï¼‰ã€‚
4. ä½†æ˜¯ MLM åˆå¯¼è‡´è®­ç»ƒé˜¶æ®µå’Œæµ‹è¯•é˜¶æ®µæ•°æ®çš„ä¸ä¸€è‡´æ€§ã€‚è¿™æ˜¯å› ä¸ºåœ¨è®­ç»ƒé˜¶æ®µï¼ŒBERT ä½¿ç”¨ `[MASK]` ç‰¹æ®Šç¬¦å·å»æ›¿æ¢è¾“å…¥åºåˆ—ä¸­çš„ä¸€ä¸ªéšæœºè¯­ç´ ï¼Œè€Œåœ¨æµ‹è¯•é˜¶æ®µå´ç§»é™¤äº†è¯¥ç‰¹æ®Šç¬¦å·ã€‚ä¸ºæ­¤è¿›è¡Œäº†ç‰¹æ®Šçš„å¤„ç†ï¼š1ï¼‰80% çš„æ¦‚ç‡æ‰§è¡Œæ›¿æ¢ï¼›2ï¼‰10% çš„æ¦‚ç‡ä½¿ç”¨ä¸€ä¸ªéšæœºå•è¯æ›¿æ¢ï¼›3ï¼‰10% çš„æ¦‚ç‡ä¿æŒä¸å˜ã€‚
5. ä¸ºäº†ä½¿ BERT èƒ½å¤Ÿé€‚åº”æ›´å¤šçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œè¿˜å¼•å…¥äº† NSP ä»»åŠ¡ã€‚

## ğŸ“šå‚è€ƒæ–‡çŒ®
1. [BERT çš„æ¼”è¿›å’Œåº”ç”¨](https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650411744&idx=2&sn=1db39446e4e91299f9ba8c1d4eeb5983&chksm=becd94ba89ba1dac221e2092cb1a12ecb1a5a704b6ae5cebd2649adc1a903355b95567ab484e&mpshare=1&scene=1&srcid=&sharer_sharetime=1572502447054&sharer_shareid=68f8b84d7a46cc216b0afdc45278d6be&key=8a4bbb55c6c79ce6c9104a6cfe5de2a3d1b8fa801c35e22e74b62948f50b6684c3f06195815e8712080977db6cec80fca5adfc95c9bc6fa848b8e68b41df13d8610e8d6c283ee2392b30de5cdae504bb&ascene=1&uin=MTQxMTUzMzk2MA%3D%3D&devicetype=Windows+10&version=62070152&lang=en&pass_ticket=pZijWLQmmCpNBDcjO4cUImTRWv1ZWLG4JENv1zUqjhXnUnShPGofPjjR%2Bkv1cozV)
2. [ä¸‡å­—é•¿æ–‡å¸¦ä½ çºµè§ˆ BERT å®¶æ—](https://zhuanlan.zhihu.com/p/145119424)
3. [A Survey on Contextual Embeddings](https://arxiv.org/pdf/2003.07278.pdf)
4. [Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/pdf/2003.08271.pdf)
5. [ä» Word Embedding åˆ° Bert æ¨¡å‹ â€” è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæŠ€æœ¯å‘å±•å²](https://zhuanlan.zhihu.com/p/49271699)

<textarea id="bibtex_input" style="display:none;">
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
</textarea>



