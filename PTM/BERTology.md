目前已经有大量的网络资源、综述总结过预训练模型，因此我打算不再赘述。综述文献可参考：

0. [huggingface transformers 源码阅读和实践](https://mp.weixin.qq.com/s/Mdrbpq0x2KUUGh6cLc-U_A)
1. [A survey on contextual embeddings](https://arxiv.org/pdf/2003.07278)
2. [Pre-trained models for natural language processing: A survey](https://arxiv.org/pdf/2003.08271.pdf)

自然语言处理有三大范式：1）非神经网络时代的完全监督学习；2）基于神经网络的完全监督学习；3）预训练，微调（pre-train，fine-tune）。[引用](https://zhuanlan.zhihu.com/p/395115779)

经过 1 年时间改进 BERT 等预训练模型后，GPT-2 (GPT-3) 和 T5 首先发现了 prompt learning。

prompt learning 有望成为自然语言处理的第四范式：预训练，提示，预测（pre-train，prompt，predict）。



