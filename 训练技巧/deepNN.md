## 为什么要 deep？
1. 深度神经网络是一种**特征递进式**的学习算法，**浅层的神经元**直接从输入数据中学习一些低层次的简单特征，例如边缘、纹理等。而**深层的特征**则基于已学习到的浅层特征继续学习更高级的特征，从计算机的角度学习深层的语义信息。
{% note default %}
如这段所说，浅层的神经元可以从数据中提取出一些低层次的特征。对于 CV 来说，首先提取图片的一部分，然后可能还可以继续通过这一部分图片进一步地提取信息。
但是对于 NLP 来说，第一层提取出的特征是低级的，那么第二层该如何从低级的特征中提取出高级的特征？从低级中提取出高级的东西，这逻辑说不通吧。所以我觉得可能可以加一层残差层（residual layer），将原始文本的特征也并入到低级特征中（可以用一个系数来权衡量级）。->Transformer
{% endnote %}
2. 深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。

可以观察下图发现，确实越deep代价越小。左边的两列显示了，层数越多代价越小。右边两列显示即使整个神经网络参数类似，但是明显越deep代价越小。同一行代表hidden layer的参数接近。那个size指的是神经网络中的参数。再看最后一行，它显示即使参数暴增到16k，代价也不是很低。图中红框还显示了2层2k的model比1层16k的model好多了。
下图是由底下的论文的作者做的实验得出的结论。
![隐藏层层数对cost的影响](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/隐藏层层数对cost的影响.jpg)

**那么为什么神经网络越深效果越好呢？**这其实归功于modularization——模块化。如下图所示，如果直接写一个model用于将4种人分类，那么可能会出现某类人的数据并不多的情况，比如长发的男生的数据可能并不多。那么分类的准确率可能会不是很高。
![解释why deep的例子](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/解释why deep的例子.jpg)

下图中先将其分类为男女以及长短发，然后再进一步分类。虽然说长发男生的数据比较少，但是男女和长短发的数据有很多，我们可以得到一个很好的模型。之后我们再叠一层用于进一步分类，此时，由于我们已经做了上一步的分类，所以新的一层可以使用上一层的特征。

上一层的分类已经把难的事情——辨别男女，辨别长短发等解决了，所以后一层只要使用少量的数据就能进行分类。

**没有使用模块化**的那个模型，它是用少量的数据硬生生地去识别长发男生。**使用模组化**的模型是先识别男女以及长短发，再通过调用前一步的特征判断。下图4个分类器区别可能只是辨别的方式不同而已，比如权重不同。输入一张图片之后第一层已经可以辨别是男还是女，长发还是短发，然后后一层经过简单的运算就可以确定了。
![模块化后](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/模块化后.jpg)

经过上面的解释，可能已经大致理解是什么意思了。但是真要讲清还有点问题，尤其是模块化怎么做。但是李宏毅老师说模块化其实是神经网络从数据中**自动**学到的。

### 更多的例子
用数电的逻辑门来举例，但是我没怎么学过数电，所以没有理解。
另一个比较贴近生活的例子，就是剪窗花。没有人会一瓣花一瓣花的去剪窗花，都是将纸先折好，然后一步剪完。这就是模块化了。

还有其他领域的人也有过解读，个人理解why deep这个问题可能到现在没有一个官方的回应，可能前辈也是误打误撞才发现deep learning很牛。
![其他领域对为什么要deep的解读](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/深度学习中的一些疑问总结/其他领域对为什么要deep的解读.jpg)

## 为什么深层神经网络难以训练？
1. 梯度消失
	梯度消失是指通过隐藏层从后向前看，梯度会变的越来越小，说明前面层的学习会显著慢于后面层的学习，所以学习会卡住，除非梯度变大。
	梯度消失的原因受到多种因素影响，例如**学习率的大小**，**网络参数的初始化**，**激活函数的边缘效应**等。在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。下图是不同隐含层的学习速率：
2. 梯度爆炸
3. 权重矩阵的退化导致模型的有效自由度减少

## 词向量乘上权重以及做梯度下降有什么意义
[本文灵感](https://mooc.study.163.com/learn/2001280005?tid=2001391038&_trace_c_p_k2_=023fecd41c524f0d9485b18d2d773f53#/learn/content?type=detail&id=2001770038)
本文疑问：
1. 词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？
2. 神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？
3. 梯度下降到底在算什么东西，在算谁的最小值？
4. 为什么它可以预测出应该填入juice？
5. 它怎么预测其他句子？

### 准备词向量
假设有这么一句话：I want a glass of orange \_\_\_.
要做的是估计划线处应该填入什么词。答案是juice。
首先我们需要一个词典——vocabulary，每个单词对应一个索引，这是通用步骤。词表大小为10000。
然后将上述的句子，从单词转成索引形式。即：
I want a glass of orange ---> 4343 9665 1 3852 6163 6257
此外每一个单词都会对应一个词向量，而词表中所有单词的词向量就组合一个词嵌入矩阵。词表以及词向量都是可以找一些预训练的，比如**GloVe**。
梳理一遍就是：
单词:索引
索引:词向量
所以可以通过单词间接地获取到词向量。关于索引对应词向量，实际上是里面没有索引的因为一个矩阵它本身就有一个属性表示索引，如第0行就是代表第0个单词，第1行就是代表第一个单词。
总而言之，我们通过单词获取索引后，就能通过该索引直接获取词向量。伪代码可以表示为：
```
index = vocabulary.get_index('want') # 索引为9665
word_vector = embedding_matrix[index, :] # 获得词向量
```
**对于词嵌入矩阵的行代表词向量，还是列代表词向量不必纠结。**你要乐意可以改成
```
word_vector = embedding_matrix[:, index] # 获得词向量
```
如果使用one hot编码来执行上述代码就是将9665转为one hot编码，即除了9665位置为1，其余位置全为0。然后$word\_vector = embedding\_matrix^T * word\_one\_hot$。这样也能得到词向量，但是由于one hot编码全是0，算起来速度太慢了。
现在有了句子“I want a glass of orange”的所有词向量，接下来要做的是将这些词向量从头到尾拼在一起，接成一个更长的向量，也就是6倍长的向量。原词向量是300维，拼接完成后是1800维。然后将这个向量输入一个神经网络中，最后经过softmax函数进行预测，预测范围是在10000个单词中，看谁的概率大。

### 意义
将词向量送入神经网络中当然还需要梯度下降进行迭代。这里会有很多疑问，

1. 词向量在神经网络隐藏层中乘一个权重，这个权重有什么用？
2. 神经网络训练完成后送入softmax中，这个输出层的权重又有什么用？
3. 梯度下降到底在算什么东西，在算谁的最小值？
4. 为什么它可以预测出应该填入juice？
5. 它怎么预测其他句子？

我进行逐一思考，本文仅为自己的理解。
首先其实有一件事很多视频没讲，可能他们认为这是一件很平常的事，所以没讲。
上述的这个步骤并不是预测步骤，而是在进行迭代，所以是一个训练步骤。人家之所以说*我们可以通过这个神经网络预测出单词为juice*，是因为逻辑上是这样的。
由于是训练步骤，所以我们有一个很重要的数据，最终结果。最终结果我们是知道的，然而我们初学者在考虑整个流程时，没把最终结果算进去，因为老师说*我们可以通过这个神经网络预测出单词为juice*，由于是*预测*，那么结果肯定没有啊。这很合乎逻辑。所以就陷入了一个思维的怪圈，**正确的逻辑是：**
1. 首先我们知道最终结果，所以当第一次迭代时，所有的权重都是随机初始化的，1、2两个问题也就没有意义了。第一次迭代完毕后，结果肯定稀巴烂，所以进行梯度下降。
2. 这里面我们又会碰到一个问题，就是梯度下降到底在算什么？其实这里的疑问来自我们的潜意识始终将句子当做文字在看，自然而然就意识不到梯度下降在干什么。而其实我们在几步之前就已经将文字转为词向量了。
词向量说白了就是一堆浮点型数字，而最终结果juice也是一个词向量，所以实际上就是将一个权重矩阵乘上一个1800维的向量，得到一个输出值（may be 激活值），然后将这个输出值和juice的向量放入代价函数中进行计算，接下来的梯度下降其实就是跟正常的步骤一样。
3. **梯度下降就是在寻找一个合适的权重矩阵使得权重矩阵乘1800维向量得到的值接近juice的向量。**
这里在解释第3个问题时，顺便也解释了第1、2个问题。**权重值实际上就是用来使得预测值和实际结果越接近越好**
4. 由于开头就说了我们实际上是知道划线处应该填juice，所以第4个问题压根不需要解答，因为我们本来就知道应该填juice，也没必要预测。之前之所以有这个问题，是因为我们潜意识觉得老师说*我们可以通过这个神经网络预测出单词为juice*。
5. 至于第5个问题如何预测其他句子。打个比方，现在预测一个新的句子：I want a glass of apple \_\_\_.
由于我们知道词嵌入矩阵是由很多单词的词向量组成的。而一个单词词向量其实就是一堆特征组成的，对比两个句子，apple和orange的词向量肯定很接近，因为它们都是水果，它们的特征（水多不多，好不好吃，是不是水果，有没有性别特征，是不是动词等）都类似。
而我们之前已经训练了一个神经网络，我们得到了所需要的权重值，我们直接把这个权重值乘上新句子的词向量，那么结果肯定和权重值乘上之前句子的词向量的结果很接近。因为它们之间的区别仅仅是apple和orange的词向量不同，并且apple和orange的词向量其实也并不是完全不同，只是略有不同。所以二者的结果自然也差不多。
最后经过softmax函数在10000个单词之中预测，输出的结果必然都是juice。

### 其他
这里还会有一个问题，那么如果需要预测的句子单词数不一致怎么办？
之前两个例子的句子都是6个单词，预测第7个单词。那要是前面有10个单词，我要预测第11个单词怎么办？
其实可以只考虑划线处前几个单词，比如只考虑划线处前4个单词，这样输入的维度就相同了。
在本文开头的参考视频里，吴恩达老师讲得很清楚了。由古圣先贤总结，一般来说取前4个单词即可，当然你自己也可以用其他办法。


## 📚参考文献
- 为什么要 deep？
	1. [吴恩达 2017course 深度学习笔记：对神经网络整体的理解#为什么使用深度表示——Why-deep-representations](https://yan624.github.io/·zcy/AI/dl/对神经网络整体的理解.html#为什么使用深度表示——Why-deep-representations)
	2. [吴恩达老师的解释](https://mooc.study.163.com/learn/2001281002?tid=2001392029&_trace_c_p_k2_=03442699ea78498a873a1dbe2fcfee40#/learn/content?type=detail&id=2001701022)