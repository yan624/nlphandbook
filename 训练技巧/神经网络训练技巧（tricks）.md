---
title: 神经网络训练技巧（tricks）
tags: [4me]
categories: [AI, liandan]
top: 1
abbrlink: 8071918e
date: 2020-04-26 19:55:20
---
{% note default %}
{% label success@gradient clipping %} {% label primary@learning rate decay %} {% label info@dropout %} {% label info@tensorboardX %} {% label primary@过拟合/欠拟合 %}
{% endnote %}

# 技巧
[@jozefowicz2015empirical] 发现在 LSTM 的 forget gate 上的 bais 设置为 1，可以缩小 LSTM 和 GRU 之间的差距。（题外话：该论文还很好地解释了梯度消失出现的原因）下面是解释：

{% quote %}
许多应用将 SLTM 的遗忘门随机地使用一个很小的值进行初始化，这在大多数应用上都能够正常工作。但是这种初始化实际上将遗忘门（**博主注**：注意是遗忘门，而不是遗忘门偏差）设置成了 0.5。这就在每一个时间步，由于 0.5 这个因素，引入了一个梯度消失的问题。当模型特别需要长期依赖时，就会导致出现问题。{% label warning@为什么会使得遗忘门变成0.5？ %}

这个问题的解决方式比较简单，可以将遗忘门的偏差设置为一个较大的数值，例如 1 或 2。通常将其设为接近 1 的值，从而启动 gradient flow。这个观点早已经被 Gers et al.(2000) 提出，但是很多从业者都不知道这点，所以我们再次强调。

如果遗忘门的偏差没有正确初始化，我们可能会错误地得出结论：LSTM 无法学习解决具有远程依赖性的问题。然而根据上面的观点，事实并非如此。
{% endquote %}

{% note warning %}
值得注意得一点是，他们还认为 GRU 作为 LSTM 的变种，与 LSTM 一样，仍然难以证明其 cell 是否设计得合理。并且据实验发现，**基于普通的初始化方式**，其几乎在所有任务上胜过 LSTM。但是正如之前所述，只要 LSTM 的 forget get bias 被初始化为 1，它们之间的差距几乎相当。

博主注：那么也就是说 GRU 的优点只有比 LSTM 的运行速度快一点
{% endnote %}

# 调参的步骤以及经验
{% note info %}
[李宏毅深度学习](https://www.bilibili.com/video/BV1Ux411S7rk?p=13) 大致地讲解了调参技巧，不过现在都是人尽皆知的方法了。
{% endnote %}
根据本人经历，记录调参的方法。

## 最初的超参数
首先你需要固定自己的 epoch 以及 batch size，然后使用尽可能大的学习率（learning rate，lr），使得你的模型在训练集上拟合的不错，即 loss 值达到可以接受的程度，并且准确率或者你的其他指标达到不错的水平。**一般我将 batch size 选为 128，epoch 选为 100。**如果初步调试后无法得到一个好的结果，可以根据需求略微地调整这三个参数。{% label danger@三个最重要的参数 %}

得到一个不错的效果意味着你的模型本身没有存在很大的问题。**但是**由于取的是尽可能大的 lr，那么你的模型在验证集上的结果可能差的离谱！这不要紧，因为之后还要调整各个超参数，此步仅仅是找 lr。最后有一点，如果经过长期调试，在训练集上均无法得到一个好的结果，那么得检查一下代码或者数据处理的是否有问题。{% label primary@验证模型是否有问题，同时得到合适的学习率 %}

要牢记一点，**超参数的好坏一定要在验证集上做评定，并且最好保证在训练集上的性能不受太大影响**。此外，**在尝试合适的学习率时，可以把验证步骤关了，因为此时的验证精度没什么用。这样做可以加快训练速度。**{% label info@一些小技巧 %}

现在假设你最终使用一套 `(lr, epoch, batch_size)` 的超参数，在训练集上得到一个还不错的结果。那么可以进行下一阶段，选取合适的 batch size（*注意此步《最初的超参数》是用来选择 lr 的，epoch 和 batch size 仅仅是一个根据经验固定的值*）。详见下节《[batch size的选择](#batch-size的选择)》。
{% note warning %}
如果在训练集上的结果还不错，那么在验证集上的结果应该不会差到训练不起来的地步。此时可以通过调整上述三个超参数使性能提高，注意，稍微调一下就行。一般来说应该不需要调整。
{% endnote %}

## batch size的选择
在看了参考文献 1 之后，我对目前我正在做的模型做了几个小实验。我以前以为 batch size 对模型的影响不大，我认为只要 batch size 设置得足够大，只要有足够大的 epoch，模型总能收敛。但是在做了这几个实验之后，我发现 batch size 似乎能够影响模型的性能。

### 几个小实验
下图是几个实验的结果，图有点乱，但是其实很好分析。只需要记住一点，曲线在上方的都是 train BLEU，曲线在下方的都是 val BLEU。训练集上的性能比验证集上的高，这很正常。

此实验，其他超参数均固定，epoch 可以暂且无视。先看上方的三条线，灰色的超参数为 `(lr, epoch, batch_size) = (0.001, ?, 64)`，而蓝红分别为 `(0.0003, ?, 32)` 和 `(0.0003, ?, 16)`。可见 batch size 为 64 的模型训练集上的 BLEU 指标比不上其它的模型。下方的验证集同理。还有一点，图中 batch size 为 32 的模型收敛要慢，这是因为 lr 不合理，设置为 0.0005 可能更合理。那么对于此任务来说，这两组参数可能都是一个不错的选择。但是我倾向于 `batch size=32` 的，因为它训练的速度更快，毕竟时间就是生命。
![不同超参数之间，性能的对比](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/不同超参数之间，性能的对比.svg)

下面给出所有实验的对比，发现还是那两组的性能最好。橙色那条线是 `(0.001, ?, 128)`。此外我还试了 `(0.002, ?, 128)`，但是这组实验直接烂掉了，模型早早地拟合完毕，但是训练集上的 BLEU 连 50% 都没。由于已知增加 batch size 的同时需要加大 lr，由上述的两种实验可知，该任务参数的上限可能就是 `(0.001, ?, 128)`。
![不同超参数之间，性能的对比2](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/不同超参数之间，性能的对比2.svg)

由上述实验可知，batch size 和 lr 还是对模型的性能有影响的。并且二者对模型的影响是具有相互作用的。这是因为在 loss function 中，唯二的超参数就是 batch size 和 lr【[1](https://zhuanlan.zhihu.com/p/64864995)】，其他的超参数是都在神经网络里。上述的实验只是一个大概的实验，其中的变化太多，无法选择到一组完美的组合，但是还是可以有经验可循的。**加大 batch size 就要加大 lr，减小 batch size 就要减小 lr，而且可以以对应倍数进行改变**【[1](https://zhuanlan.zhihu.com/p/64864995)，[2](https://arxiv.org/pdf/1706.02677.pdf)，[3](https://arxiv.org/pdf/1705.08741.pdf)】。

假设经过《[调参的步骤以及经验](#调参的步骤以及经验)》之后，现在你的超参数为 `(0.001, 100, 64)`。那么你接下来可选的超参数组合为：`(0.002, 100, 128), (0.0005, 100, 32), (0.00025, 100, 16)`。具体的参数值可以按照自己的习惯进行改写，例如 0.00025 可以改成 0.0003。

总而言之，知道这一点之后，有关 lr 和 batch size 的调整就不需要像无头苍蝇一样乱调了。可以选定大概 5 组参数，直接放在服务器上训练。然后选择一个最合适的范围，再进行微小的调整。或者如果嫌麻烦，可以直接选择 5 组里最好的。

### 总结
上述的实验只是做个补充，总的来说：
1. 大的 batch size 可以减少训练时间，模型更稳定（*关于模型更稳定这点，我没有在论文里读到过，只是取自【[1](https://zhuanlan.zhihu.com/p/64864995)】*）。因为批次越大，所需要迭代的次数就要少，训练速度肯定要快。
2. batch size 不是越大越好

### 参考资料
1. [【AI 不惑境】学习率和 batchsize 如何影响模型的性能？](https://zhuanlan.zhihu.com/p/64864995)
2. [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/pdf/1706.02677.pdf)
3. [Train longer, generalize better: closing the generalization gap in large batch training of neural networks](https://arxiv.org/pdf/1705.08741.pdf)

## 如何处理过拟合
参考《过拟合处理》

## loss 值并不能作为判断模型性能的指标
有一个让我印象深刻的案例，我发现验证集上的 loss 值很高，但是在验证集上的准确率还不错。我认为**loss 值并不能作为判断模型性能的指标**。

例如我的实验，在训练集上 loss 值的大小收敛到 [0.1, 1]，但是在验证集上 loss 值高达 [25, 50]，所以在最开始我一直认为模型在验证集上的性能肯定极差。现在假设以 acc 作为评分指标，在训练集上可能有 60%+，而在验证集上我猜测连 1% 都没有。由于这种错误的思维，其实我并没有编写计算 acc 的代码，因为我觉得这个模型有问题，所以我懒得耗费时间编这个代码。

最后我受不了了，编写了计算 acc 的代码，发现两个数据集上的性能居然类似，大致分别为 68% 和 48%。下面是基于直觉的分析，假设我们的目标多分类任务，预测 `['我', '是', '鱼']`。

由于在训练集上的 loss 值很低，所以自然而然地可以想到它的预测水平要高，假设预测 `'我'` 的概率为 0.9，那么对应的损失值为 $-\log(0.9)$（负对数似然）。而在验证集上，假设预测 `'我'` 的概率为 0.6，那么对应的损失值为 $-\log(0.6)$。可以看出显然验证集上的损失值要大得多，但是他们的结果是一样的，都预测正确了。注：这里的损失值其实相差还是不大，但是对于语言生成任务来说，损失值是要累加的，假设一句话有 30 个字，那么累加之后，差距就大了。

## GPU内存不够以及随机采样
如果你实验室的机器不好，打算使用随机采样的方法使得在小样本上测试超参数。**那么请记得 train/val/test 的比例一定要与原数据集的比例一样**。数据集一般都是由制作者划分好比例的，比如你得到的数据集 train/val/test 的数据量为 12472/900/900。现在你想只训练 6600 条数据，以此提高找到超参数的效率，那么你必须做到以下几点：

按照比例缩放：$\frac{6600}{12472} \approx 0.529$，那么你所取得的 val/test 数据集合一定要是 $0.529 \times 900 \approx 476$ 条。简单来说，你从总的训练集中取多少比例，也要到验证集和测试集里按这个比例取。不能“*哦，总共就 900 份，我不如都算作验证集吧*”，**这样会导致数据的分布极其的不平衡**！试想一下，数据的制作者肯定会尽量使得三份数据集的分布尽可能的接近，并且其中的句子肯定也是长短不一的。结果你从训练集中随机取出 6000 份，导致了这些数据的分布和原来不一样，一个直观的解释就是模型无法捕获长句子的信息，因为你的新训练集和训练集中长句子所占的比例不一样。而你还偏偏用原来那个分布的验证集！

一定要使用随机采样的方法！不要直接使用 python 的切片！否则也会导致你的数据集分布不平衡。使用 python 内置的 `random.choice()` 可以很简单地实现随机采样。

## 关于Adam的一个bug
如果需要对模型使用权重衰减的策略，**在 Pytorch 中不要使用 Adam 算法，有 bug**。听说目前流行的深度学习框架都有 bug。请改用 AdamW 算法。甚至在一开始就可以选择 AdamW，因为 Adam 和 AdamW 是一样的，区别仅仅是在权重衰减步骤上有所不同。

# 对于如何融合两个向量的思考
在训练神经网络时，有些情况会有两个张量，但是网络的输入只有一个。例如在训练 Bi-LSTM 时，会分别得到正向以及逆向的输出。这两个张量的特征我们都需要利用到，那么该如何融合它们的特征呢？

常用的做法就是**1）直接相加**；**2）或者拼接两个张量**。

**个人比较喜欢第二种方法**，因为我自学习神经网络开始，一直将一个向量的各个维度视作数据的各个特征。而张量拼接的做法，直观上来看，其实就是**将两个张量的特征合并**。个人认为这更符合我的逻辑。反之，张量相加好像体现不出特征的奇妙之处。

当然这样做也各有优缺点，**张量拼接的方式导致了维度倍增**，而相加的方式还是维持原来的维度。但是张量拼接之后可以接一个前馈神经网络（甚至可以接多层感知机，让网络更深），进行降维。总得来说，张量拼接的方式虽然使得维数倍增，但是现在这个年代，这个维数也可以接受，只要的维数不是高的离谱就行。同时**方法 2 后接一个前馈神经网络**，这又进一步**对两个张量进行了特征提取**，用一句话形容就是“取其精华，去其糟粕”。前馈网络对张量的降维可以理解为删去了无用的特征。

其次，方法 1 和方法 2 其实也是类似的。一般方法二需要接一个线性变换，即 $C = f(\begin{pmatrix}M \\ N\end{pmatrix})$。而方法一可以公式化为：$D = \begin{pmatrix}1 & 1\end{pmatrix} \cdot \begin{pmatrix}M \\ N\end{pmatrix} + 0$。说白了就是方法一相比于方法二，权重是 1，偏差是 0。也就是说方法一的做法**没有利用神经网络自动为两个张量分配合适的权重**。

最后，其实还有另一个做法。先对两个张量做一次线性变换，再相加。这其实和方法二完全一样，观察下式。实际上，$W_c b_c$ 都可以拆成两个折半的矩阵/向量，例如 $W_c = \begin{pmatrix}W_{c_1} & W_{c_2} \end{pmatrix}$。拆开之后就是 $C = f(M) + F(N)$，这其实就是先进行线性变换再相加。
$$
f(x) = W \cdot x + b \\
C = f(\begin{pmatrix}M \\ N\end{pmatrix}) = W_c \cdot \begin{pmatrix}M \\ N\end{pmatrix} + b_c
$$

## 参考资料
1. [神经网络中对需要 concat 的特征进行线性变换然后相加是否好于直接 concat?](https://www.zhihu.com/question/389912594/answer/1178054600)
2. 以后有机会再补充

# 过拟合处理
{% note warning %}
奇怪的事写在前面。

我碰到了一种情况，训练集上拟合的很好（loss 约为 1），但是在验证集上很差（loss 约为 280+）。不断地调整代码以及更换学习率，得到的效果均不明显。

我给出其中一种可能：**两种数据集的分布不一样**。例如我的问题，在给定 10000 条训练集样本和 900 条验证集样本的情况下。由于 NLP 任务句子不等长的特性，所以我选择先将句子排序使得同一批次中的数据等长，注意我没有选择填充 `<PAD>` 这种方法。

然后**我取了 6600 条训练集样本，在取验证集样本时，由于它的数据量较小，所以我全要了，问题就出在这**。我的训练集样本是取的 [1400: 8000]，所以这个区间内的句子长度比较平均，由于我事先排序的原因，所以极短的句子和极长的句子我都没取到。但是原验证集和原训练集的分布是一样的！所以 900 条验证集中包含了极长的句子和极短的句子，这导致了现在的两个数据集分布不一样了！**解决办法是使用随机采样**。*不要排序之后直接切片获得数据，而是应该是先随机采样，再排序*。

但是处理完之后结果依旧很烂！现在正在调试学习率。待续，后面补充。
{% endnote %}

当过拟合时，可以考虑以下的方法，过拟合的另一种说法是高方差（high variance）。

## 增加数据集的大小
如下图所示，紫红色和蓝色曲线是验证集上的 loss。蓝色曲线意味着我从 4400 条数据增加到了 6600 条。明显可以看到过拟合略有缓解。但是训练集上的 loss 增加了，灰色那条线是增加数据量后的训练集 loss。
![增加更多的数据](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/add_more_data.svg '增加更多的数据')

下图也是一个例子，相比于上个模型还训练的有问题，这个模型已经训练的还可以了。可以看到红色曲线比蓝色的要平缓一点，因为数据集增加，训练集上的 loss 会相应增加。再看上面的橙色和深蓝色，深蓝色是增加数据量之后的 loss 曲线，略微降低。
![增加更多的数据2](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/add more data2.svg)
## 加大批次大小或减小模型复杂度
如下图所示，中间的两条橙色和蓝色曲线是批次大小从 128 增加到 224 的结果。橙色曲线为 val loss，可以看到橙色曲线相比于上面那条蓝色的 val loss，已经略有下降，这代表着过拟合的程度略有降低。而中间的蓝色曲线代表 train loss，相比于下面的灰色 train loss 曲线有些许上升，这代表着模型在训练集上的泛化能力有所下降。
![加大批次大小](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/more_larger_batch_size.svg)

下图为减小隐藏状态的例子，中间的绿色以及红色曲线是隐藏状态从 512 减到 400 的结果，可以看到验证集上的 loss 略有下降（略微的缓解过拟合），训练集上的 loss 略有上升。
![减小隐藏状态大小](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/reduce_hidden_size.svg)

# 集成学习Ensemble

# 可视化训练结果
以前一直用 matplotlib 来画图，现在用了 tensorboardX 之后，感觉人瞬间就爽了，以下为教程。无法启动看 2，启动后网页无法显示看 3，代码不会写看 1。

参考资料：
1. [官方文档](https://tensorboardx.readthedocs.io/en/latest/tutorial.html#what-is-tensorboard-x)
2. [tensorboard OSError: [Errno 22] Invalid argument错误处理](https://blog.csdn.net/qq_40605167/article/details/95761885)
3. [tensorboard生成的网址打不开的解决方法](https://blog.csdn.net/weixin_44135282/article/details/86156961)
4. [详解PyTorch项目使用TensorboardX进行训练可视化](https://blog.csdn.net/bigbennyguo/article/details/87956434)

# 其他的调参经验

## 拟合异常分析
### val loss上升，val acc也上升
{% note warning %}
这里的 acc 只是一种比较抽象的说法，换成 BLEU 等指标也是可以的。
{% endnote %}
使用 seq2seq 训练一个对话模型时，发现 val loss 始终很高，用尽了办法也没办法让它下去。train loss 一般都在 0.x - 3 之间，BLEU 也在 80%-90% 之间。但是 val loss 就是大概在 80 以上，而 BLEU 在 30 % 左右。在下面几个章节的图片中可以明显的看到二者的 gap 非常大。


#### 参考资料
1. [验证集 loss 上升，准确率却上升该如何理解？](https://www.zhihu.com/question/318399418/answer/1202932315)
<!-- more -->

### 训练集和验证集的性能或者loss相差巨大
此问题与下一章《train acc很大，gap非常大》的问题类似，或者说完全一样。无论怎么调整参数或者加什么正则化方法，都无法使得验证集 loss 减小，或者准确率上升。这意味着需要增加数据集，其他的方法都不管用。具体看下图，我增加了大约已被数据集后，黄色验证集曲线（7872 条数据）比绿色验证集曲线(4000 条)下降很多。
![加大数据集后，val_loss明显下降](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/加大数据集后，val_loss明显下降.svg)

#### train acc很大，gap非常大
在训练过程中出现了一种情况，train acc 94% 左右，但是 val acc 只有 24% 左右，test acc 也是 24% 左右。很多人认为这是**过拟合**状态，因为模型在训练集上拟合的很好，但在验证集上拟合的不好，这两者之间的 gap 很大。也就是**高方差**。
![train acc很大，gap也很大](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/train acc很大，gap也很大.svg)

以下是对此现象的分析：
1. 再增加 **epoch** 已经没用了，因为该模型的参数已经很好的拟合了数据集，使得准确率接近 100%。
{% cq %}说明训练集中的几乎所有信息（不论是对泛化有用的信息还是训练集中的噪声）都全部被模型学习到了。（[来源](https://www.zhihu.com/question/65200055)）{% endcq %}

2. 改变（无论增大还是缩小） rnn 的**隐藏状态**也不会有很大改善了。比如说此文章的模型，我尝试选择隐藏状态大小为 (100, 115, 125, 128)，结果如下所示，可以看到增加减少隐藏状态大小，对 train acc 根本无影响，对 val acc 也只有一点影响：
{% cq %}
hidden size | batch | train acc | val acc | test acc
------------|-------|-----------|---------|---------
100 | 32 | 94.42% | 20.74% | 21.28%
115 | 32 | 94.42% | 22.80% | 23.72%
125 | 32 | 94.42% | 24.95% | 24.78%
128 | 32 | 94.42% | 21.20% | 22.58%
125 | 128 | 98.47% | 19.13% | 20.37%
{% endcq %}
{% note info %}
但是我认为这种情况**不算过拟合**（暂时不看 `batch=128` 那组）。因为过拟合指的是模型在训练集上过度地拟合了数据特征，于是在验证集上出现个比较离群的数据样本，就会得到很差的结果。
如果把训练集拥有的特征集合定义为 A，验证集拥有的特征集合定义为 B，那么 $C=A \cap B, D = A \cup B - C$，其中 C 代表的是未被神经网络拟合的很小的一部分特征，D 代表的是已经被神经网络拟合的特征（注：这里的拟合并不一定要完全的拟合，在拟合线的附近即可）。神经网络要做的是同时拟合 A 与 B 的所有特征。
但是随着 epoch 的增加，A 中的所有特征肯定被拟合的越来越好，而 B 中有些已经被拟合的特征却越来越远离拟合线，所以导致了 C 中的特征越来越多。（可以通过逻辑回归来先思考，验证集数据原本在拟合线附近，但是增加 epoch 或者增加网络参数等操作导致过拟合，原本在拟合线附近的数据点反而偏离了）
**所以训练集和验证集中的数据的分布应该是这样的：训练集和验证集中有很多数据分布类似（对应即使过拟合 val acc 也不会下降到 0%），但是两个数据集中有些数据只有部分特征的分布是类似的（对应于过拟合），最后两个数据集汇总有一部分数据完全无关联（对应于不管怎么拟合，val acc 始终不可能到 100%）。**
话说回来文中的情况，明显训练集和验证集的 acc 到达顶部之后一直处于平稳的状态，增加 epoch 或者增加网络参数都没有使得 acc 增加或下降，这就意味这 C 中的特征没有变化。
**所以这种情况应该是这样的：训练集和验证集中的数据有少量是类似的（对应于 val acc 拥有 20+%），其余大部分，可能有超过 60% 的数据都没有关联（对应于不管怎么训练 val acc 始终只有 20+%），最后有两个数据集中一小部分数据可能有部分特征重叠（对应于随着 epoch 的增加，val acc 有略微的波动）。**
{% endnote %}

对这种情况，一般出现在利用随机采样快速训练模型，只需要加大数据量就行了。所以以后再用小数据量训练时只需要将参数调整到满意的地步就不需要再继续调整了，直接换大数据量。
最后对于 `batch = 128` 的那组，train acc 上升了，其他的下降了，有过拟合嫌疑，但是幅度不是很大。

## loss与acc之间的关系
[深度学习中 loss 和 accuracy 的关系？](https://www.zhihu.com/question/264892967/answer/833656253)

## 关于epoch
如果你调试了很多参数之后，模型在**训练集**上的性能没有很大改善，**可以尝试大幅度增加 epoch**。当然你也可以在一开始就使用很大的 epoch。因为有些模型很奇怪，可能 loss 曲线已经很平滑了，但是在某个 epoch 之后，loss 值突然暴跌。例如，下图是训练了一段时间的 loss 值，我卡了四五天，一直以为 loss 不会下降了。
![smooth loss](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/smooth_loss.svg)

但是当你加大 epoch 之后，突然发现它又开始下降了。（注：这两条曲线有点不一样，是因为我略微地调整了代码，但是这已经足够说明问题了。经过我的实验，这里的 loss 值下降与我的代码调整无关）
![slumping loss](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/slumping_loss.svg)

这可能不太明显，我们把它缩放一下，发现真的又开始下降了。
![scaled slumping loss](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/scaled_slumping_loss.svg)

## 加大lr，train loss不一定更快收敛
我有碰到一个情况，**加大学习率后，训练集上的 loss 反而下降的更慢了！**图片我放在了下面，其中深蓝色的曲线代表着学习率是要大的那个模型。我的猜测是，学习率加大后，在某一个时间段的 epoch 中，模型一直在尝试走到山底，但是由于学习率过大，导致它一直来回振荡。而学习率小点，就正好一脚跨下去了。从图中也可以发现，在一开始，大学习率的模型确实要下降的快点，但是后来被小学习率的模型反超了。
![增加 lr，loss 反而下降更慢](https://blog-content-1256924128.cos.ap-shanghai.myqcloud.com/zcy/神经网络训练技巧（tricks）/增加lr，loss反而下降更慢.svg)







