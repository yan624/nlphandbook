文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领域的重要研究方向。最简单的有独热编码（one-hot encoding）、词袋模型（bag of word）；基于统计的方法有 TF-IDF、主题模型；基于神经网络的方法有词嵌入模型、预训练模型。

## 词袋模型
词袋模型（bag of word，BOW）将每篇文章看成是一袋词，忽略每个单词的出现顺序。具体来说，BOW 将所有文档以词为单位切分，得到词表。然后每条文本输入可以表示为一个长向量，其中每一个维度代表一个单词，而该维所对应的权重反应了这个词在输入中的重要程度。

最简单的权重表示方法是**词频**。考虑到一些无意义单词出现频率较高，例如“的”，一般会降低它们的权重。因此更常用的方法是 TF-IDF，公式为：

$$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \text{IDF}(t)
$$

其中 $\text{TF}(t,d)$ 为单词 $t$ 在文档 $d$ 中出现的频率，即词频，公式为 

$$\text{TF}(t, d) = \frac{t 在 d 中的出现次数}{d 的单词数}
$$

$\text{IDF}(t)$ 是**逆文档频率**，衡量单词 $t$ 对所表达语义的重要程度，公式为：

$$\text{IDF}(t) = \log(\frac{文章总数}{包含 t 的文章总数 + 1})
$$


举一个基于*单词出现次数*的例子，有文档：`{'你好棒棒', '你好棒呀', '你好吗'}`

1. 首先建立字典 `{'你':0, '好':1, '棒':2, '呀': 3, '吗': 4}`
2. 那么可以得到两个长向量 `[[1, 1, 2, 0, 0], [1, 1, 1, 1, 0], [1, 1, 0, 0, 1]]`

基于 TF-IDF 的 BOW 模型可以使用类似的做法。观察上述例子，我们采取的是以单词为单位进行切分的方法。这可能丢失了部分语义，可以使用 **n-gram 算法**进一步优化该方法，例如将 '你好' 作为一个词汇，而不是 '你' 和 '好'。这既保留了语义，还可以减少词表大小。缺点是当“你”单独出现时，它会成为一个表外词（out-of-vocabulary，OOV）。N-gram 算法见《[基础知识·统计语言模型](基础知识/语言模型?id=统计语言模型)》一章。

此外，可以发现对于文档 '你好棒棒' 和 '你好吗'，其中充斥着许多 0。对于该问题，我们可以使用分布式表示（Distributed Representation）来处理，例如**词嵌入模型**。关于分布式表示的翻译可以参考[文章](https://zhuanlan.zhihu.com/p/22386230)。

值得注意的是 BOW 只是一个概念，是 TF-IDF 算法利用了 BOW，除此之外还可以用 TextRank、LDA 等。

?> 在参考文献里，有一个人评论如果某一篇文章题为《“的”字的分析与应用》，那么这算法还有用吗？我觉得对于这个问题，首先作者在写的时候应该会用引号将“的”围起来，要不然本身就有歧义。因此可以将“‘的’”作为一个单词。

## 主题模型
主题模型用于从语料库中发现有代表性的主题，并且能够计算出每篇文章的主题分布。该模型比较复杂，放至《[基础知识·主题模型](基础知识/主题模型)》一章总结。常见的算法有 TF-IDF、TextRank、LDA 等。

## 词嵌入模型
18 年末的 BERT 模型无疑是将 NLP 带入了预训练时代。但是在此之前，NLP 领域中，其实早已经有了预训练的概念。它就是 word embedding。word embedding 其实是更早以前（2003 年）神经语言模型（NNLM）的副产物，但是当时并没有引起多大的关注。直到 2013 年，深度学习进军 NLP 领域时，才焕发异彩。NNLM 在《[语言模型·语言模型](基础知识/语言模型)》一章总结。

谷歌于 2013 年发布 Word2Vec 工具包，值得注意的是 Word2Vec 只是一个工具包，但是同时它也是该词向量的代称，并且也可以算做一种单词转为向量的算法名称，所以初学者在接触它时可能会感到困惑。

接下来我简略地介绍一下 word2vec 与 NNLM 的不同之处。

1. **目的不同**：word2vec 的目的是产生词向量；NNLM 的目的是实现一个神经语言模型，它同样具有词向量，只不过这个词向量是副产品。
2. **模型算法不同**：为了产生词向量，word2vec 使用了两种算法，即 CBOW 和 skip-gram。其中 CBOW 使用单词 $w_t$ 的上下文来预测其本身；而 skip-gram 与之相反，使用单词 $w_t$ 来预测其上下文。由于 NNLM 的主要目的是构建语言模型，所以它使用的算法是传统的条件概率，即已知前面若干词的情况下，预测下一个单词的概率。
3. **模型结构不同**：为了更快地训练词向量，word2vec 舍弃了隐藏层，那么它的模型架构为 input -> output。而 NNLM 还是使用了隐藏层，即 input -> hidden -> output。

此外，由于 word2vec 出现的时间处于深度学习的早期，彼时，GPU 还未在深度学习领域中大量投入使用。所以它还附带了两种高效的解码算法，即 hierarchical softmax 和 negative sampling。hierarchical softmax 主要是为了加速 softmax 从而设计出的一个近似算法，思想是使用层次的二元分类逼近多元分类，不过由于现在 GPU、TPU 大行其道，该算法已经不太常见。

此后还出了不少的词向量算法，例如 2014 年的 GloVe，2016 年的 fasttext。但是无论是哪种算法都存在着一个致命的问题，就是无法处理一词多义。**后起之秀 ELMo、BERT 就是来缓解这一问题的。**

最后补充一点，个人认为 BERT 使用的 MLM 模型与 CBOW 算法类似，都是使用周围词来预测其中的某一个词。

### word2vec
2013 年谷歌发布了一个名为 Word2Vec 的工具包，标志着 NLP 领域正式走进了预训练词向量的时代。

关于 word2vec 这个算法，这篇[博客](https://www.cnblogs.com/peghoty/p/3857839.html)已经讲得很清楚了。值得注意的是 word2vec 发表的时间是 2013 年，而该文发表时间是 2014 年，因此算是深度学习刚萌芽的时候。博主用了相对现在比较古老的知识来进行讲解。处于深度学习时代的我们可能会忽略这些远古知识，这篇文章也算是帮我们复习了一遍。

其中介绍的背景知识点包括：贝叶斯公式、哈夫曼树；统计语言模型、 n-gram 模型、神经语言模型。然后就是讲解了 word2vec。最后讲了一些源码细节（注：这部分好像也有点用，因为我在知乎上看到，有个外国小哥认为论文中的 word2vec 和实际发布的 word2vec 算法工具包完全就不一样，论文里面简化了太多东西）。

防止这篇文章挂掉，我把该文章的名字也写下，网上应该有其他人做了备份。博文名称为**《word2vec 中的数学原理详解**》，作者分别发表在了 CSDN 和博客园。

文章中，第 1、2、3 章都是前置知识，如果已经对自己掌握的知识有自信的话，可以跳过。

#### CBOW
CBOW 算法的本质是通过周围词来预测中间词。优化的是函数 $p(w | context(w))$，其中 $context(w)$ 代表 $w$ 周围的词，具体窗口大小取决于自己。

#### skip-gram
skip-gram 算法的本质是通过中间词来预测周围词。优化的是函数 $p(context(w) | w)$，其中 $context(w)$ 代表 $w$ 周围的词，具体窗口大小取决于自己。

#### 训练方法

##### Hierarchical Softmax
hierarchical softmax 在输出层会**事先**构建一棵哈夫曼树，这里面涉及到了很多 tricks，具体可以看最后一章《源码细节》。简答来说，就是通过计算词频预先构建一棵哈夫曼树。

模型在最后的输出层本质上是执行了若干个**二元分类**。一般来说，模型在输出层会计算 $z = a \cdot W$，其中 $W$ 代表向量维度乘类别大小。由于 hierarchical softmax 执行二分类，因此 $W \in \mathbb{R}^{D \times 2}$，$W \in \mathbb{R}^{D \times 1}$ 也可以。

而 hierarchical softmax 优化的对象本质上是一个哈夫曼编码，例如字“猫”的哈夫曼编码为 `1001`，那么就需要执行 4 次二元分类，每次优化的对象为 1、0、0、1。**最后把四次分类的结果相乘**就是 $p(猫 | context(猫)) = \prod_i p_i$ 其中 $p_i$ 代表所做的所有二元分类。

了解损失函数之后，就可以对其优化了。与其它的分类任务一样，我们使用交叉熵损失函数。剩下的就是优化部分了，说白了就是求导，可以看上面提到的文章。

##### Negative Sampling
Negative Sampling 涉及到太多计算，直接看原文吧。需要注意的是，初看 negative sampling 感觉很难，因为放眼望去全是公式，但是静下心来看个 5-10 分钟，还是感觉挺简单的。（PS：本人数学学渣）

#### 训练词向量得到的 accuracy
word2vec 的 accuracy 貌似没用，参考[讨论](https://www.zhihu.com/question/271782463)。我用 keras 搭的 CBOW 模型的 accuracy 极低（1% 不到）。

### GloVe
这篇 CSDN 的[文章](https://blog.csdn.net/coderTC/article/details/73864097)应该已经讲得很清楚了。下面我做一点点的补充。

定义词共现矩阵 $X$，其中 $X_{ij}$ 代表单词 $j$ 出现在上下文单词 $i$ 附近的次数。令 $X_i = \sum_k X_{ik}$ 表示任意单词出现在上下文单词 $i$ 附近的次数，简单来说就是累加第 $i$ 行所有的值。最后，令 $P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}$ 作为单词 $j$ 出现在上下文单词 $i$ 附近的概率，即**共现概率**。这很直观，$X_i$ 是所有单词出现在 $i$ 附近的次数，$X_{ij}$ 是 $j$ 出现在 $i$ 附近的次数。注意这里的附近指的是 n-gram 模型，具体可以自己设置。

**共现概率的概念很重要。**接下来用一个例子解释一下它到底有啥用。如下表：

Probability and Ratio | k=solid | k=gas | k=water | k=fashion
----------------------|---------|-------|---------|----------
P(k \| ice) | $1.9\times 10^{-4}$ | $6.6\times 10^{-5}$ | $3.0\times 10^{-3}$ | $1.7\times 10^{-5}$
P(k \| steam) | $2.2\times 10^{-5}$ | $7.8\times 10^{-4}$ | $2.2\times 10^{-3}$ | $1.8\times 10^{-5}$
P(k \| ice)/P(k \| steam) | 8.9 | $8.5\times 10^{-2}$ | 1.36 | 0.96

表头不言自明，来看表中的第一二三行。第一、二行代表共现概率 $P_{ij}$，根据公式，值越大代表两个单词越有关联，也就是说共现的概率越大。对于 ice 而言，solid 和 water 的共现概率比较大；对于 steam 而言，gas 和 water 的比较大。利用探查词 $k$ 计算二者**共现概率的比值**可以检验这两个单词的关系。对于 $k=\text{solid}$ 与 ice 相关而与 steam 无关，比值会很大；对于 $k=\text{gas}$ 与 steam 相关而与 ice 无关，比值会很小。对于类似 water 或 fashion 的单词 $k$，要么与二者都有关，要么都无关，比值会趋向于 1。

**作者 [@Pennington2014] 认为词向量学习的起点应该是共现概率的比值，而不是概率本身。**基于以上观点，对于已知的单词 $i,j,k$ 的比值 $r = \frac{P_{ik}}{P_{jk}}$，即 $i,j$ 的关系，我们应该创建一个函数 $\hat{r} = f(i,j,k)$，使得 $\hat{r}$ 尽可能地接近 $r$。由于我们的目的是学习词向量，因此函数的输入自然是随机初始化的向量，然后使用梯度下降算法更新它们。

!> 单词 $i,j,k$ 的比值是已知的，是因为它们是从我们语料库中算出来的。同时，单词 $i$ 和 $j$ 之间的关系也是已知的。

现在剩下的问题就是，函数 $f$ 是什么？损失函数是什么？这些就去看那篇文章或者直接看原文吧。

!> 最后，GloVe 不是神经网络模型。

### FastText
> fasttext 是 facebook 开源的一个词向量与文本分类工具，在 2016 年开源，典型应用场景是“带监督的文本分类问题”。提供简单而高效的文本分类和表征学习的方法，性能比肩深度学习而且速度更快。[5]

FastText 模型是一个具有秩约束（rank constraint）的简易线性分类器，分类器只有一个隐藏层，不过 FastText 还有另一个隐藏层，即词嵌入矩阵，记为 $E$。FastText 的输入是文档的 n 元词袋（bag of n-gram），记为 $x$，那么 $E \cdot x$ 就可以得到文档词向量的加和。这实际上是一个查表操作。FastText 的总体结构与 CBOW 类似。*不知道这个秩约束是啥意思，可能指 hierarchical softmax。*

FastText 的概率分布由 softmax 计算得到，然后使用 SGD 优化普通的交叉熵损失函数，公式可以表示为：

$$
\begin{aligned}
	L & = - \frac{1}{N} \sum^N_{n=1} y_n \log(f(linear(E \cdot x_n))) \\
	& = - \frac{1}{N} \sum^N_{n=1} y_n \log(\text{softmax}(A \cdot E \cdot x_n)))
\end{aligned}
$$

其中 $N$ 代表小批量的样本，$y_n$ 代表文档的标签，$f$ 是 $\text{softmax}$，$\text{linear}$ 可以简化为一个权重矩阵 $A$。*$Ex_n$ 可以取均值。*

以上是 FastText 的基本结构以及训练方式。接下来展开其中的细节。

**首先**，输入选择 n 元词袋是因为作者考虑到词袋模型对词序是不变的，然而明确地考虑词序通常代价又很大，例如使用 LSTM。为此使用 n 元词袋作为额外的特征捕获局部词序的一些部分信息。**FastText 使用的是二元词袋**，原文指出可以使用更高元略微地增加性能，例如三元可以在 sogou 上提升 0.3%。

**其次**，使用 *hashing trick* 配合与 [@mikolov2011strategies] 一样的 hashing function 以减少存储 n-grams 的内存。

**最后**，为了加速模型训练和推理，FastText 使用了 hierarchical softmax。

> FastText 模型（2016）与 ACL15 的论文类似，Deep Unordered Composition Rivals Syntactic Methods for Text Classification。得到的结论也类似，**对于简单的任务，使用简单的结构即可**。当然，如果有些任务对词序很敏感，那么可能还是需要更强的模型，例如 RNN，Transformer。

## 预训练模型
预训练模型是一个使用深度学习技术从大规模语料库中学习到的语言模型。可以将该语言模型视为一个函数，输入一段长度为 $L$ 的文本可以得到 $L$ 个词向量。与词向量模型不同，这些词向量是基于上下文的、动态的。

预训练模型分类复杂，运用了大量技术且发展迅速，现已自成一派。放至《[预训练模型](PTM/index)》一章总结。

## 参考文献
- 词袋模型
	1. [TF-IDF 与余弦相似性的应用（一）：自动提取关键词](https://www.ruanyifeng.com/blog/2013/03/tf-idf.html)
- 词嵌入模型
	1. [word2vec 数学原理](https://www.cnblogs.com/peghoty/p/3857839.html)（[CSDN 版](https://blog.csdn.net/itplus/article/details/37969519)）
	2. [word2vec 原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html)（这个其实基本上就是上一篇文章的复制版，但是加了一点对各种编程语言实现的评价）
	3. [词向量，LDA，word2vec 三者的关系是什么？](https://www.zhihu.com/question/40309730/answer/86453469)
	4. [理解 GloVe 模型（Global vectors for word representation）](https://blog.csdn.net/coderTC/article/details/73864097)
	5. [FastText： 快速的文本分类器](https://blog.csdn.net/john_bh/article/details/79268850)

<textarea id="bibtex_input" style="display:none;">
@InProceedings{Pennington2014,
  author    = {Jeffrey Pennington and Richard Socher and Christopher Manning},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  title     = {Glove: Global Vectors for Word Representation},
  year      = {2014},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/d14-1162},
  file      = {:pdf/14_EMNLP-1162_GloVe-Global Vectors for Word Representation.pdf:PDF},
  groups    = {pretrain},
}
@inproceedings{mikolov2011strategies,
  title={Strategies for training large scale neural network language models},
  author={Mikolov, Tom{\'a}{\v{s}} and Deoras, Anoop and Povey, Daniel and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan},
  booktitle={2011 IEEE Workshop on Automatic Speech Recognition \& Understanding},
  pages={196--201},
  year={2011},
  organization={IEEE}
}
</textarea>
















