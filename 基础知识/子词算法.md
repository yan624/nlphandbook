在自然语言处理领域，模型的输入通常由一系列的符号组成，可以是一条语句，也可以是拼接的多条语句。为了使模型可以处理文本数据，一般而言，需要将所有单词映射为 id，并为其创建一个独一无二的向量。实际上，我们会将文本预先映射为其对应的 id。在计算时，模型根据这些 id 就可以获得对应的向量，然后进行矩阵运算。

在现实场景中，由于大部分单词并不频繁出现，让模型学习这些低频词的特征有点不切实际。此外，由于每个单词都拥有一个向量表征，系统内存/显存也不足以支撑存储大量的表征。我们通常会使用一个特殊符号 `<UNK>` 表示这些低频词，不过这样丢失了部分语义信息以及句法信息。

这些低频单词被称为表外词（Out of Vocabulary，OOV），科研领域已有不少研究旨在解决这一难题，但是除了字符级嵌入之外，第一个突破性算法是子词算法[3]。

子词算法旨在使用一系列符号表示一个低频词，例如“hugingface”可以被表示为 `['hugging', '##face']`。以下介绍几种常用的子词算法：BPE（Byte Pair Encoding）、WordPiece、ULM、BBPE（byte-level BPE）。

## BPE
BPE 算法是一个简单的数据压缩算法，由 [@gage1994new](#gage1994new) 首次提出。[@sennrich2015neural](#sennrich2015neural) 将 BPE 引入神经机器翻译（Neural Machine Translation，NMT）任务，他们的工作证明 NMT 有希望通过子词单元（subword units）编码（稀有）单词，这种做法比使用*大规模词表* 和 *back-off dictionaries* 的做法更简单且更高效。

> 他们将 BPE 算法应用于 NMT 基于以下假设：将稀有单词切分成合适的子词单元足以让 NMT 网络学习显而易见的翻译，例如命名体、组合词、音译词等，并泛化这些知识从而使得模型能够翻译及产生未知的单词。

> 有人将 BERT 称为新纪元的开始，但是我认为 BPE 是这场竞赛中的一匹黑马，这是因为其成功应用于现代 NLP 模型之中，而相应的关注却得非所望。[3]

NMT-BPE 算法合并最频繁出现的字符或者字符序列，而不是 byte（原始的 BPE 算法）。

!> 虽然网络上充斥着各种 BPE 算法的解读以及论文笔记，但是我认为大部分文章仅仅是对论文的翻译。本文根据 BPE 算法源码，描述以下两项内容：如何将算法应用在语料上以及编码解码步骤。

### 如何将算法应用在语料上
?> 以下算法的第一、二步来自对[源码](https://github.com/rsennrich/subword-nmt/)的总结，第三步来自 [@provilkov2019bpe]。`merge table` 的叫法也来自该论文，源码应该将其称作 `most_frequent`（训练）或者 `bpe_codes`（编码）。

BPE 算法的一个核心问题是如何对已知的语料分词。首先需要明确一点，BPE 算法有两大步：第一是学习 BPE，第二是将学习到的知识（codes）应用在语料库上，即编码。以下为算法步骤，第 1、2 步是学习步骤，第 3 步是编码步骤。在许多教程、笔记中通常只有前两步，第三步被省略。在开始讲解算法之前，先介绍一下名词。字符词表（character vocabulary）包括字母、其它语言字符和其它特殊符号，例如空格。符号词表（symbol vocabulary）的范围更大，包括字符词表、更多的特殊符号以及人为定义的子词单元。

1. 初始化符号词表和 `merge table`：使用字符词表以及一个特殊符号`</w>`初始化符号词表，然后将所有单词切分为字符序列外加一个结束符`</w>`；`merge table` 初始化为空集。（在下面会举例说明一下）
2. 执行 BPE 以此得到 `merge table`：重复以下步骤，直至满足预设的词表大小（即合并次数）或者符号对的出现次数为 1。
	1. 统计所有符号对的数量；
	2. 合并最频繁出现的符号对：每轮迭代合并一对最频繁的符号，例如 ('es', 't') -> 'est'，并且更新词表；
	3. 将上一步合并的符号对加入 `merge table`。
3. 对语料分词：遍历每一条语句中的每一个单词
	1. 重复以下步骤，直至无法构建待合并符号对 `merge_pairs` 或单词长度等于 1（也就是说子词被还原了）；
		1. 构建待合并符号对 `merge_pairs`：将子词序列切割为多个符号对，如果 `merge table` 存在某个符号对，则将其加入 `merge_pairs`；
		2. 按照**优先级**合并 `merge_pairs` 中的某个符号对。

在原论文或其它地方最常见的可能是以下的词表：

```
vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}
```

看上去有点奇怪，因为常见的词表应该是一个字对应一个 id，而不是像上面这样。此外为什么里面的每个字母是分开的？其实该词表省略了一步，就是用字符词表将每个单词切割。而呈现这种形式可能是因为这样比较高效，因为在语料库中存在很多重复的单词，这种形式可以让每个单词只需被切割一次。

详细解释一下第 3 步：例如有单词“merger”，3.1 首先将其拆解为“m-e-r-g-e-r\</w\>”字符序列，**其中 - 代表符号对存在于 `merge table`**，符号对表示为 ('m', 'e'), ('e', 'r'), $\cdots$。3.2 根据优先级合并符号对，假设合并('e', 'r')，则字符序列被合并为“m-er g-er\</w\>”，注意此时假设 `merge table` 中不存在('er', 'g')的合并操作。重复以上步骤，最后即可得到“mer ger\</w\>”。以此类推，对语料中的所有语句分词。  

**优先级**指的是符号对加入 `merge table` 的顺序，越早加入，优先级越高。请参考[代码](https://github.com/rsennrich/subword-nmt/blob/234923ed53c19f17a4456f1316f14dd9e033712b/subword_nmt/apply_bpe.py#L273)，其中 `pairs` 由[该行](https://github.com/rsennrich/subword-nmt/blob/234923ed53c19f17a4456f1316f14dd9e033712b/subword_nmt/apply_bpe.py#L267)生成，而 `bpe_codes[pair]` 由[该行](https://github.com/rsennrich/subword-nmt/blob/234923ed53c19f17a4456f1316f14dd9e033712b/subword_nmt/apply_bpe.py#L57)生成。`self.bpe_codes` 就是以符号对加入 `merge table` 的顺序进行排列。

以下对几个可能有的问题做出解释：

!> 是不是每个被合并的符号对都要加入 `merge table`？如果是，不会出异常吗？例如“('e', 's')”和“('es', 't')”。

是。但是不会有异常。我们假设“('e', 's')”比“('es', 't')”先加入 `merge table`，且有单词“l o w e s t”，并已得到该单词的所有符号对。由于我们在 `merge table` 中先找到符号对“('e', 's')”，因此单词合并为“l o w es t”；又按顺序找到“('es', 't')”，因此合并为“l o w est”。可以看到没有出现异常。

!> 为什么将单词逐个地分割为字符序列，而不是将语料切分为字符（字母）序列，然后在整个语料上迭代地合并最频繁的符号对？因为后者的时间复杂度很高，如果有 N 个符号，则后者是一个 $O(N^2)$ 的操作。

!> NMT-BPE 论文声明*符号词表大小等于初始词表大小加上合并操作次数*，但是我认为貌似无法得到这个结果。因为每次执行合并操作之后，只会出现以下三种情况，而这三种情况都是随即发生的。
1. 词表大小 + 1：两个待合并符号既可以独立存在，又可以连续出现；（两个符号依旧存在，但多一个新符号）
2. 词表大小 + 0：两个待合并符号中的一个必定与另一个同时出现，而后者却可以独立存在；（前者被消除）
3. 词表大小 - 1：两个待合并符号只会连续出现。（二合并成一）

### 编码与解码
解码步骤相对简单，只需要遍历序列，然后合并每一个子词，直至其以 `</w>` 结尾或者序列结束为止。编码步骤的计算复杂度非常高，实际上我们会预先编码所有单词，然后将编码结果存储在文件中备用。[2]

NMT-BPE 论文中提供了两种编码：一种是 n-grams，它直接将单词切分为 n-grams 形式的字符序列而不是执行合并操作（上一节第 3 步），如果词表中不存在某个 n-gram，则记为`<UNK>`；第二种就是 bpe 算法（上节步骤 3）。

后来在网络上看到了另一种编码方式[2, 6]，具体步骤为：1）将符号词表中的符号以其长度进行逆序排序；2）从长至短遍历每个符号，并与某个单词进行匹配，直至单词被拆解为由多个符号组成的序列；3）如果单词中有部分结构无法在词表中找到对应的符号，则使用 `<UNK>` 表示。

### joint BPE
joint BPE 算法将源词表（source vocab）和目标词表（target vocab）合并。其余过程与普通 BPE 算法一致。

!> NMT-BPE 使用的是 joint BPE。

### Subword Regularization
[@kudo2018subword](#kudo2018subword) 认为基于 BPE 算法一个单词可能会有多种分词，甚至在相同词表的情况下都会发生。例如：“Hello”有如下几种分词：1）Hell/o；2）H/ello；3）He/llo；4）He/l/l/o；5）H/el/l/o。他们提出：是否能够将这种分词的模糊性作为一种噪音以此提升 NMT 健壮性。*与 dropout 类似。*

与其他 NMT 系统相同，他们将 NMT 建模为 seq2seq，然后在一组语料 $\mathit{D}=\{(x^s, y^s)\}^{|\mathit{D}|}_{s=1}$ 上（*通常使用 $\mathit{D}$ 的子集，即 mini-batch*）优化交叉熵损失函数：

$$\mathcal{L}(\theta) = \sum^{|D|}_{s=1} \log P(y^s|x^s; \theta)
$$

**Subword Regularization 的具体做法**是：假设对 $\mathit{X}$ 和 $\mathit{Y}$ 分词可以得到多条子词序列及其对应概率 $P(x|\mathit{X})$ 和 $P(y|\mathit{Y})$，则基于此假设优化**边缘似然**（marginalized likelihood）的参数 $\theta$：

$$\mathcal{L}_{marginal}(\theta) = \sum^{|D|}_{s=1} \mathbb{E}_{\begin{aligned} x \sim P(x|\mathit{X^s}) \\ y \sim P(y|\mathit{Y^s})\end{aligned}} [\log P(y|x; \theta)]
$$

想要精确地优化上式是不现实的，因为潜在的分词随序列长度呈指数增长。*每个低频单词都可能有多种分词，枚举所有单词的所有分词是不切实际的。*所以他们使用有限的 $\mathit{k}$ 个序列近似上式，各自采样于 $P(x|\mathit{X})$ 和 $P(y|\mathit{Y})$：

$$
\begin{aligned}
	\mathcal{L}_{marginal}(\theta) & \cong \frac{1}{k^2} \sum^{|D|}_{s=1} \sum^k_{i=1} \sum^k_{j=1} [\log P(y_j|x_i; \theta)] \\
	x_i & \sim P(x|\mathit{X^s}), y_j \sim P(y|\mathit{Y^s})
\end{aligned}
$$

> For the sake of simplicity, we use k = 1... When we have a sufficient number of iterations, subword sampling is executed via the data sampling of online training, which yields a good approximation of (3) even if k = 1. It should be noted, however, that the subword sequence is sampled on-the-fly for each parameter update.

#### 解码
在解码时，只有源句 $\mathit{X}$。1）**one-best decoding**：最直接的方式是选择概率最大的片段 $x^* = \argmax_x P(x|\mathit{X})$。2） **n-best decoding**：也可以使用 $P(x|\mathit{X})$ 的 n 个最佳分词。具体来说，给定 n 个最佳分词 $(x_1, \cdots, x_n)$，我们使用以下公式选择最佳的结果 $y^*$：

$$score(x,y) = \frac{\log P(y|x)}{|y|^{\lambda}}
$$

其中 $|y|$ 是序列长度，$\lambda \in \mathbb{R}^+$ 是惩罚较短语句的参数。

#### 使用语言模型分词
为了更好地进行采样，提出了一种新的分词算法：unigram language model（ULM），它能够输出多种分词结果的概率。详见 [ULM](#ULM) 章节。

### BPE dropout
[@provilkov2019bpe](#provilkov2019bpe) 认为虽然在相同词表中一个单词有多种分词，但是 BPE 只选择一种分词，这意味着对于每个单词模型只能观察到一种分词情况。因此模型可能**无法发挥其在形态学上的潜力**，**无法学习单词的组合以及无法对分词误差产生足够的健壮性**。

为了处理该问题，一个比较自然的方式是利用多种分词候选这一特性。这由 [@kudo2018subword](#kudo2018subword) 最早提出，将其作为一种子词正则化（Subword Regularization）手段。由于 BPE 只能产生单一分词，为了实现子词正则化，他们提出了 ULM——一种新的子词分词算法。然而引入此算法的步骤相当复杂：它需要训练一个独立的一元语言模型，使用 EM 和 Viterbi 算法，并且禁止使用传统的 BPE。

BPE dropout 利用 BPE 本身的随机性，展示了想要实现子词正则化并不需要抛弃 BPE。它对分词步骤进行修改，在每个合并步骤中，一些合并以 $p$ 的概率被丢弃。具体算法为：

1. 初始化符号词表和 `merge table`：使用字符词表以及一个特殊符号`</w>`初始化符号词表，然后将所有单词切分为字符序列外加一个结束符`</w>`；`merge table` 初始化为空集。（在下面会举例说明一下）
2. 执行 BPE 以此得到 `merge table`：重复以下步骤，直至满足预设的词表大小或者符号对的出现次数为 1。
	1. 统计所有符号对的数量；
	2. 合并最频繁出现的符号对：每轮迭代合并一对最频繁的符号，例如 ('es', 't') -> 'est'，并且更新词表；
	3. 将上一步合并的符号对加入 `merge table`。
3. 对语料分词：遍历每一条语句中的每一个单词
	1. 重复以下步骤，直至无法构建待合并符号对 `merge_pairs` 或单词长度等于 1（也就是说子词被还原了）；
		1. 构建待合并符号对 `merge_pairs`：将子词序列切割为多个符号对，如果 `merge table` 存在某个符号对，同时它没有被丢弃，则将其加入 `merge_pairs`，丢弃概率为 $p$；（**此为与 BPE 唯一不同的步骤**）
		2. 按照**优先级**合并 `merge_pairs` 中的某个符号对。

如果 $p=0$，那么等价于标准 BPE；如果 $p=1$，那么算法会将单词分词成字符。通常在训练阶段 $p=0.1$，推理阶段 $p=0$。（原文第五节讨论了如何选择 $p$）

?> 以上步骤的含义已经在[《如何将算法应用在语料上》](#如何将算法应用在语料上)一节详细描述过，下面对论文中的例子进行解释（以 b.1 为例）：现有字符序列“u-n-r-e-l-a-t-e-d”，使用 BPE dropout 后合并为“u-n re-l-a-t-e-d”，请注意每一步都会重新构建符号对，而 'n'、're' 之间没有连接线的原因是 `merge table` 中没有 ('n', 're' ) 符号对。

## WordPiece
WordPiece 算法最早由谷歌团队 [[@schuster2012japanese]](#schuster2012japanese) 为解决日语/韩语分词问题而提出，后被用于谷歌[[@wu2016google]](#wu2016google)的神经机器翻译系统。

## ULM
[@kudo2018subword](#kudo2018subword) 等人提出 Subword Regularization，为了实现这一正则化算法，引入新的分词算法——基于**一元语言模型**（Unigram language model，**ULM**）的分词算法。一元语言模型的假设是每个子词均独立出现，那么子词序列 $x = (x_1, \cdots, x_M)$ 的概率可以表示为子词出现概率 $p(x_i)$ 的乘积：

$$P(x) = \prod^M_{i=1} p(x_i), \, \forall i \, x_i \in \mathcal{V}, \, \sum_{x \in \mathcal{V}} p(x) = 1
$$

其中 $\mathcal{V}$ 是预先确定的词表。正如 [Subword Regularization](#Subword-Regularization) 章节中所描述，基于 BPE 算法，一个单词可能会有多种分词，那么对于输入语句 $X$ 最可能的分词 $x^*$ 可以通过以下公式计算：

$$x^* = \argmax_{x \in \mathcal{S}(X)} P(x)
$$

其中 $\mathcal{S}(X)$ 是输入语句 $X$ 的候选分词集合，由于分词结果空间过于庞大，可以使用 **Viterbi 算法**获得 $x^*$，其只选择 $n$ 种可能。

如果词表 $\mathcal{V}$ 已知，则子词出现概率 $p(x_i)$ 可以通过 **EM 算法**估计，其假定将 $p(x_i)$ 视为隐藏变量，然后最大化以下的边缘似然 $\mathcal{L}$。

$$\mathcal{L} = \sum^{|D|}_{s=1} \log(P(X^s)) = \sum^{|D|}_{s=1} \log(\sum_{x \in \mathcal{S}(X^s)} P(x))
$$

!> 要想理解以上所描述内容，需要对以下两种算法有所了解：Viterbi 算法和 EM 算法。

但是在真实环境中，词表 $\mathcal{V}$ 是未知的，因此采用以下算法迭代地构造词表以及计算分词的概率。

1. 从训练语料中创建一个最够大的种子词表；
2. 确定期望的子词词表大小，例如 20000；
3. 重复以下步骤，直到 $|\mathcal{V}|$ 达到预设的大小。
	1. 固定词表，使用 EM 算法优化 $p(x)$；
	2. 计算每个子词 $x_i$ 的损失值 $loss_i$，其代表从当前词表中删除 $x_i$，似然 $\mathcal{L}$ 减低的可能性；
	3. 基于 $loss_i$ 排序符号，保留前 $\eta$% 个子词（例如 $\eta = 80$）。为了避免出现表外词，推荐保留单一字符。

## BBPE

## 参考文献
1. [BPE 算法地址](https://github.com/rsennrich/subword-nmt)
2. [Byte Pair Encoding](https://leimao.github.io/blog/Byte-Pair-Encoding/)
3. [Byte Pair Encoding — The Dark Horse of Modern NLP](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10)
4. [A Deep Dive into the Wonderful World of Preprocessing in NLP](https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/)
5. [3 subword algorithms help to improve your NLP model performance](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46)
6. [深入理解 NLP Subword 算法 ：BPE、WordPiece、ULM](https://zhuanlan.zhihu.com/p/86965595)

<textarea id="bibtex_input" style="display:none;">
@article{gage1994new,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={McPherson, KS: R \& D Publications, c1987-1994.}
}
@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}
@article{kudo2018subword,
  title={Subword regularization: Improving neural network translation models with multiple subword candidates},
  author={Kudo, Taku},
  journal={arXiv preprint arXiv:1804.10959},
  year={2018}
}
@article{provilkov2019bpe,
  title={Bpe-dropout: Simple and effective subword regularization},
  author={Provilkov, Ivan and Emelianenko, Dmitrii and Voita, Elena},
  journal={arXiv preprint arXiv:1910.13267},
  year={2019}
}
@inproceedings{schuster2012japanese,
  title={Japanese and korean voice search},
  author={Schuster, Mike and Nakajima, Kaisuke},
  booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5149--5152},
  year={2012},
  organization={IEEE}
}@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}
</textarea>

