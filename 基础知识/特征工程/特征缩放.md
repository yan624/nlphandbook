常用的方法主要有：

1. **最值缩放**（min-max scaling），亦称最值归一化、线性函数归一化。对原始数据进行线性变换，使结果映射到 $[0, 1]$ 的范围。
$$X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}
$$
其中 $X$ 是原始数据，$X_{max}$、$X_{min}$ 分别是数据中的最大值和最小值。从式中可以发现最大值变为了 1，最小值变为了 0。[适用于本来就分布在有限范围内的数据](https://www.zhihu.com/question/26546711/answer/62085061)，数据较为稳定，不存在异常值；如果数据中包含异常值或噪音，例如一个非常大的值，那么可能会影响缩放的质量。如果想要缩放到 $[a, b]$，使用
$$X_{norm} = a + \frac{(X - X_{min})(b - a)}{X_{max} - X_{min}}
$$
2. **均值标准化**（mean normalization），亦称均值归一化。将原数据映射到 $[-1,1]$。
$$X_{norm} = \frac{X - X_{mean}}{X_{max} - X_{min}}
$$
3. **均值方差标准化**（Z-score normalization），亦称 Z-score 标准化、均值方差归一化、standardization（标准化）。将原始数据映射到均值为 0，标准差为 1 的分布上。
$$z = \frac{x - \mu}{\sigma}
$$
[适用于分布没有明显边界的情况，受 outlier 影响也较小。](https://www.zhihu.com/question/26546711/answer/62085061)
4. Scaling to unit length
$$x' = \frac{x}{||x||}
$$

!> 注意：这四种特征缩放方法的名称很混乱，通常第 1 种被称为归一化，第 3 种被称为标准化（standardization）。此外，[normalization 的翻译具有归一化和标准化两种意思，standardization 特有标准化一种意思](https://www.zhihu.com/question/20467170/answer/1644110030)，这就更乱了！既然已经这么乱了，那干脆就不要区分了，归一化和标准化随便说。

## 数值型特征
数值型特征常常具有不同的[量纲](https://baike.baidu.com/item/量纲)，例如分析一个人的身高和体重对健康的影响，如果使用米和千克作为单位，那么二者的数值范围差距比较大，体重特征可能对模型的贡献可能会更多。综上，很有必要缩放数值型特征。

缩放数值型特征可以 1）消除不同特征之间量纲的影响，使它们处于同一数量级（大致相同的数值区间内）、具有可比性 [@zhuge2018]。2）加速训练。

## 类别型特征
类别型特征（categorical feature）主要指性别、血型等在有限选项内取值的特征，它们通常以字符串形式出现。可以使用以下三种方法预处理：

1. 序号编码：常用于具有大小关系的数据
2. 独热编码：常用于不具有大小关系的数据
3. 二进制编码：相比于独热编码，更节省存储空间

## 参考文献
<textarea id="bibtex_input" style="display:none;">
@book{zhuge2018,
	author = {诸葛越 and 葫芦娃}, 
	year = {2018}, 
	title = {百面机器学习}, 
	publisher = {人民邮电出版社}
} 
</textarea>

- 特征缩放
	1. [为什么 feature scaling 会使 gradient descent 的收敛更好?](https://www.zhihu.com/question/37129350)
	2. [特征工程中的「归一化」有什么作用？](https://www.zhihu.com/question/20455227/answer/197897298)



