语言模型在很久以前（多久以前，待查证）就已经出现了，它的主要目的是计算一个句子的概率。

## 统计语言模型
> 统计语言模型最先用于解决语音识别问题。计算机需要知道一个文字序列是否能够成人类可以理解且具有意义的句子。——《数学之美》，第 3 章。

语言模型假设一门语言中所有可能的句子服从一个分布，每个可能存在的句子的概率加起来等于 1。当然，由于在一门语言中有很多句子，所以即使是一个可能存在的句子，它的概率值也是很小的，但是比起一个不太可能存在的句子，它的概率就远远大得多。

更严格的描述是：假定 $S$ 表示一个有意义的句子，它由一串顺序排列的单词 $w_1, w_2, \cdots, w_n$ 组成，其中 $n$ 是句子的长度，句子 $S$ 在一门语言中存在的概率为 $P(S)$。那么如何计算它呢？最简单的办法是：统计人类有史以来所有的句子，然后计算句子 $S$ 存在的次数，用它除以所有句子的个数。当然这方法肯定行不通。

我们可以用数学的方式计算它。已知 $P(S) = P(w_1, w_2, \cdots, w_n)$，那么利用条件概率的公式，将其展开得到：
$$
\begin{aligned}
P(w_1, w_2, \cdots, w_n)
	& = P(w_1) \cdot P(w_2 | w_1) \cdots P(w_n | w_1, w_2, \cdots, w_{n-1}) \\
	& = \prod^n_{i=1} P(w_i | w_1, w_2, \cdots, w_{i-1}) \tag{1}\\
\end{aligned}
$$

> 上式很容易得到，但是为了照顾不会概率论的童鞋，我还是略微地推导一下。假设我们将 $P(S)$ 简化一下，修改为 $P(ABCD)$，则根据条件概率公式 $P(X|Y) = \frac{P(XY)}{P(Y)} \implies P(XY) = P(X|Y) \cdot P(Y)$，同理也可以计算出：
> $$P(ABCD) = P(D|ABC) \cdot P(ABC) = P(D|ABC) \cdot \underbrace{P(C|AB) \cdot P(AB)}_{P(ABC)} = \cdots$$
>
> 这里可能有人会有疑惑，为什么是 $P(ABCD) = P(D|ABC) \cdot P(ABC)$ 呢？而不是 $P(ABCD) = P(A|BCD) \cdot P(BCD)$ 或者 $P(ABCD) = P(ABC|D) \cdot P(D)$ 等其他公式。还记得我们在做自然语言处理吗？例如 `S = “我喜欢你” = ABCD`，在“我喜欢”（ABC）的条件下，计算“你”（D）的概率是有意义的。但是在“喜欢你”（BCD）的条件下，计算“我”（A），或者在“你”（D）的条件下，计算“我喜欢”（ABC）的概率都是完全没意义的。因为语言是线性的，你说一句话，永远都是按“我喜欢你”的顺序说。而不会先说“你”，再说“我喜欢”。

继续。显然这样的方法已经从人肉穷举法，转换为了数学上的公式。但是现在有一个问题，计算 $w_n$ 的概率需要依赖前 $n-1$ 个单词，这其实还是需要用人肉穷举法。那么有没有一个办法，可以使得使用穷举法时，更加方便点呢？

上上个世纪有一个人叫马尔可夫，他提出了一个方法，说穿了就不值钱了。就是偷个懒，在计算单词 $w_i$ 的条件概率时，不考虑前 $i-1$ 个词，只考虑前**一个**单词 $w_{i-1}$。那么上式（1）就转换成了：
$$
\begin{aligned}
P(w_1, w_2, \cdots, w_n)
	& = P(w_1) \cdot P(w_2 | w_1) \cdots P(w_n | w_{n-1}) \\
	& = \prod^n_{i=1} P(w_i | w_{i-1}) \tag{2} \\
\end{aligned}
$$
这就是 Bigram Model（二元模型）。当然也可以假设与前面的多个词有关，那么就是 N-gram Model（N 元模型）了。**那么如何估算一个 $P(w_i | w_{i-1})$ 呢？**

根据条件概率公式可得：
$$P(w_i | w_{i-1}) = \frac{P(w_{i-1} w_i)}{P(w_{i-1})} \tag{3}
$$

我们只需要分别计算分子和分母的概率即可获得条件概率的值。按照最早之前的思路，我们只需要统计有史以来的所有句子，再统计 $w_{i-1} w_i$ 词组出现的次数即可获得 $P(w_{i-1} w_i)$。但是，这在之前说过了，是不可能的。那么我们可以使用一个简化的方法。

一般来说，在做一个系统之前，我们都会收集一个语料库（corpus），我们拿这个语料库充当有史以来的所有句子。这样就可以统计了。接下来我们以 $Count(X)$ 来表示 $X$ 出现的次数，$Count(\#)$ 表示语料库中的所有可能的词组或词，则出现词组 $w_{i-1} w_i$ 的概率为：
$$f(w_{i-1} w_i) = \frac{Count(w_{i-1} w_i)}{Count(\#)} \tag{4}
$$

词 $w_{i-1}$ 出现的概率 $f(w_{i-1})$ 类似。根据**大数定理**，当实验接近无穷时，这个比例就会越来越接近实际概率。所以，
$$
P(w_i | w_{i-1}) = \frac{P(w_{i-1} w_i)}{P(w_{i-1})} \approx \frac{f(w_{i-1} w_i)}{f(w_{i-1})} = \frac{Count(w_{i-1} w_i)}{Count(w_{i-1})} \tag{5}
$$

### N-gram与平滑技术
上述一个词 $w_i$ 只与前一个词 $w_{i-1}$ 有关，讲得是 Bigram Model，但是这似乎有点太过简化了。如果更复杂一点，一个词 $w_i$ 与前 $N-1$ 个词有关，那么这就是 **$N-1$ 阶马尔科夫假设**，即 N-gram 模型。一般 N 的取值都很小，一般最大只取到 4。

?> 为什么只取到 4 呢？可以查阅资料。简单来说就是复杂度呈指数上升。

此外，N-gram 模型依然无法完全覆盖所有的可能。所以后来还出来了一些新的可以捕获长期依赖的方法，将在下一节**《[神经语言模型](#神经语言模型)》**中叙述。这些方法在现在已经如雷贯耳了，比如 LSTM。

现在说平滑技术，说穿了也不值钱了。

在我们计算 $P(w_i | w_{i-1}) = \frac{Count(w_{i-1} w_i)}{Count(w_{i-1})}$ 时，如果 $Count(w_{i-1}) = 0$ 怎么办？要知道分母是不能为 0 的。此外如果正好一个词出现的次数过少，而导致它无法近似地表示其概率，应该怎么办？

平滑技术可查阅资料（《数学之美》、《自然语言处理综论》等），此处暂时不讲。

### N-gram的用途
20世纪80年代至90年代初,n-gram技术被广泛地用来进行文本压缩,检查拼写错误,加速字符串查找,文献语种识别。90年代,该技术又在自然语言处理自动化领域得到新的应用,如自动分类,自动索引,超链的自动生成,文献检索,无分隔符语言文本的切分等。  

目前N-gram最为有用的就是自然语言的自动分类功能。基于n-gram的自动分类方法有两大类,一类是人工干预的分类(Classification),又称分类;一类是无人工干预的分类(Clustering),又称聚类。

## 神经语言模型
什么是神经语言模型？在 21 世纪 20 年代，我想已经不需要解释了。RNN，LSTM，GRU 的大名都如雷贯耳。那么其与统计语言模型有什么区别呢？

在计算统计语言模型的概率时，我们是使用的人肉穷举法。那么计算 $P(S)$ 是否有更简单的方法呢？并且之前用的是二元模型，能否使用一个方法捕获长期依赖，而不仅仅是单词的前一个依赖？

深度学习技术将其变成了可能。再计算 $P(S)$ 时，我们可以使用 LSTM 捕获一整个句子的依赖，而不需要使用二元模型，得到一个近似解。其概率也不再需要统计得到，我们直接将单词转换为词向量，经过 LSTM 后，再经过 softmax，直接得到词表中每一个单词的概率。

那么具体怎么做呢？我记得 b 站上李宏毅机器学习视频里好像讲到过。无非是在每一个时间步都做一次条件概率计算。例如第一个时间步，输入 `<SOS>`，则计算 $P(A|<SOS>)$ 的条件概率，第二个时间步则计算 $P(B|<SOS>A)$。以至最后 $P(Z|<SOS>A \cdots Y)$，那么只需要把每个时间步输出的概率相乘就是整句句子的概率 $P(<SOS>A \cdots Z<EOS>)$。

神经语言模型大致就是这么个意思。RNN、LSTM、GRU 等模型将在**《[特征提取器](特征提取器)》**中描述。

{% note warning %}
需要注意的是，在早前其实神经语言模型也可以使用感知机（前馈神经网络）进行建模。由于其本身架构的限制，所以依旧需要使用 n-gram 模型，但是优点是抛弃了传统语言模型的统计方法。word2vec 就是用浅层的神经网络建立的语言模型（对于其是否是语言模型貌似还存在着争议，但是不妨碍在这里被拿来举例）。
{% endnote %}

## SLM和NNLM异同
- 异
	+ SLM 基于马尔科夫假设，只考虑一个单词前面的 n 个词；NNLM 考虑所有词。
	+ SLM 使用大数定理估计；NNLM 使用神经网络。
- 同：都可以计算一个句子的概率。

## 参考文献
- 统计语言模型
	1. 《数学之美》，第三章
	2. [神经语言模型和统计语言模型有啥异同点？](https://www.zhihu.com/question/29456588)
	3. [N-gram 的原理、用途和研究](http://blog.sciencenet.cn/blog-713101-797384.html)